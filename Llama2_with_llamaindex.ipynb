{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cb946a606d114ff28ecf5b360c81dd40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0164dfaa69464489a51e4630bdd669fa",
              "IPY_MODEL_0f17ec0893fa4380a37aa7d657f9565b",
              "IPY_MODEL_402200f1e27a4e0aa98f2bf0be3ff496"
            ],
            "layout": "IPY_MODEL_dd08c0240e0841799a6c215461b7ad41"
          }
        },
        "0164dfaa69464489a51e4630bdd669fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f70184cfad442df9baf28c4ca3cab5f",
            "placeholder": "​",
            "style": "IPY_MODEL_9f9c70cb370f4d9c991f78dd525d81a5",
            "value": "config.json: 100%"
          }
        },
        "0f17ec0893fa4380a37aa7d657f9565b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22d28d6933f64e0280347aafafd03e42",
            "max": 614,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b93997fcbd2d4f07a328fd63ceb8ceb1",
            "value": 614
          }
        },
        "402200f1e27a4e0aa98f2bf0be3ff496": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64fa7ce5e6044cfab105dd4493030130",
            "placeholder": "​",
            "style": "IPY_MODEL_b6d2f5aad89c4f0082496a48307ec0e9",
            "value": " 614/614 [00:00&lt;00:00, 44.0kB/s]"
          }
        },
        "dd08c0240e0841799a6c215461b7ad41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f70184cfad442df9baf28c4ca3cab5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f9c70cb370f4d9c991f78dd525d81a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22d28d6933f64e0280347aafafd03e42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b93997fcbd2d4f07a328fd63ceb8ceb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "64fa7ce5e6044cfab105dd4493030130": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6d2f5aad89c4f0082496a48307ec0e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ba772bbc2b1478096db7b1bc44c69f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_86f1868295bc4c60a48954c6a01e2321",
              "IPY_MODEL_5614601f3b1f4aa784494ace6c49b03f",
              "IPY_MODEL_a6659dec177b4c49bfa87721fd5fc6c7"
            ],
            "layout": "IPY_MODEL_b882e534eb774061a5c1ae981a1f2867"
          }
        },
        "86f1868295bc4c60a48954c6a01e2321": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6578105ba2804da88bb315cf09e215ae",
            "placeholder": "​",
            "style": "IPY_MODEL_d342696744724ede85ba8f6d082dd66d",
            "value": "model.safetensors.index.json: 100%"
          }
        },
        "5614601f3b1f4aa784494ace6c49b03f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_898ccd6e85164d83840d907e92ef8a0e",
            "max": 26788,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7583f6088a53439a895452992542b7cc",
            "value": 26788
          }
        },
        "a6659dec177b4c49bfa87721fd5fc6c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_866bc7e5873f44429f3c936b31729869",
            "placeholder": "​",
            "style": "IPY_MODEL_83236ac70dd941138664f088ef7eb7cc",
            "value": " 26.8k/26.8k [00:00&lt;00:00, 1.66MB/s]"
          }
        },
        "b882e534eb774061a5c1ae981a1f2867": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6578105ba2804da88bb315cf09e215ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d342696744724ede85ba8f6d082dd66d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "898ccd6e85164d83840d907e92ef8a0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7583f6088a53439a895452992542b7cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "866bc7e5873f44429f3c936b31729869": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83236ac70dd941138664f088ef7eb7cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "869f1b0de6294e70b48ca0d520a96e06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dc26823500214cc085c37bf1f5de1444",
              "IPY_MODEL_9251cccf6e554926962bb8cceaf0af72",
              "IPY_MODEL_53cee1b1eb8f4a558f45625c82438879"
            ],
            "layout": "IPY_MODEL_747e234a9e694edc9a9c9e842c20fc26"
          }
        },
        "dc26823500214cc085c37bf1f5de1444": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bf0b78b6d5345aaa4d60d395a471788",
            "placeholder": "​",
            "style": "IPY_MODEL_aa997935c3bc41389e59f5d2ebef12df",
            "value": "Downloading shards: 100%"
          }
        },
        "9251cccf6e554926962bb8cceaf0af72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2aaa72e50c044fc2aee495ee220223e0",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8270bd7cf97644f3928a64da78fce2c0",
            "value": 2
          }
        },
        "53cee1b1eb8f4a558f45625c82438879": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29b13c7f5e654969b5e2da3db9011dea",
            "placeholder": "​",
            "style": "IPY_MODEL_ba42cd53bbdd4400afb331fa10206c52",
            "value": " 2/2 [01:42&lt;00:00, 46.71s/it]"
          }
        },
        "747e234a9e694edc9a9c9e842c20fc26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bf0b78b6d5345aaa4d60d395a471788": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa997935c3bc41389e59f5d2ebef12df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2aaa72e50c044fc2aee495ee220223e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8270bd7cf97644f3928a64da78fce2c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "29b13c7f5e654969b5e2da3db9011dea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba42cd53bbdd4400afb331fa10206c52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e063d50ab9d9434e81aca0b2dd9c07a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b37212aa28045b8b5c06447c4fe0d3a",
              "IPY_MODEL_f8ec695136c243f9a773bb0a685775c9",
              "IPY_MODEL_70fd435dd0dd4bf6a6d2bcc1e2f20344"
            ],
            "layout": "IPY_MODEL_9353a274951c437293d83ffb954eed90"
          }
        },
        "1b37212aa28045b8b5c06447c4fe0d3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2b852f61e6643a394f3b25f0de4c541",
            "placeholder": "​",
            "style": "IPY_MODEL_936a4f9d5e2c4f4cbcf3aa1acd25d978",
            "value": "model-00001-of-00002.safetensors: 100%"
          }
        },
        "f8ec695136c243f9a773bb0a685775c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f4aec26fca2475dba6fdbd499836f64",
            "max": 9976576152,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e6099b66b10247e58906158487a1db55",
            "value": 9976576152
          }
        },
        "70fd435dd0dd4bf6a6d2bcc1e2f20344": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a20864cffaf8409bbc1d08af41483ad4",
            "placeholder": "​",
            "style": "IPY_MODEL_8fc9d10ac6b943e0a21dd757025240f6",
            "value": " 9.98G/9.98G [01:15&lt;00:00, 243MB/s]"
          }
        },
        "9353a274951c437293d83ffb954eed90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2b852f61e6643a394f3b25f0de4c541": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "936a4f9d5e2c4f4cbcf3aa1acd25d978": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f4aec26fca2475dba6fdbd499836f64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6099b66b10247e58906158487a1db55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a20864cffaf8409bbc1d08af41483ad4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fc9d10ac6b943e0a21dd757025240f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "affd5c74654e43839946b1a98110a201": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6cf83d42770b413fa8cc6e8cd1f71e95",
              "IPY_MODEL_58b0649a165b42cf888afbaba04d6ab4",
              "IPY_MODEL_874e96254eaa4b978d51081f3cad491d"
            ],
            "layout": "IPY_MODEL_7969158f4f1a4b8aa1241e35796100d2"
          }
        },
        "6cf83d42770b413fa8cc6e8cd1f71e95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5320c1c3bc8642189fac59b22e6ff692",
            "placeholder": "​",
            "style": "IPY_MODEL_684c1045f8d04138bed95ae6c33051eb",
            "value": "model-00002-of-00002.safetensors: 100%"
          }
        },
        "58b0649a165b42cf888afbaba04d6ab4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8f0f67e3f994916af349fe6658108cb",
            "max": 3500296424,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_722a812a5b02486dbe49e43604f9eb5c",
            "value": 3500296424
          }
        },
        "874e96254eaa4b978d51081f3cad491d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80cd85becbb748358ad192ad246fde93",
            "placeholder": "​",
            "style": "IPY_MODEL_c5f0d33ad8e144d8927f365c3d954738",
            "value": " 3.50G/3.50G [00:25&lt;00:00, 134MB/s]"
          }
        },
        "7969158f4f1a4b8aa1241e35796100d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5320c1c3bc8642189fac59b22e6ff692": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "684c1045f8d04138bed95ae6c33051eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8f0f67e3f994916af349fe6658108cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "722a812a5b02486dbe49e43604f9eb5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "80cd85becbb748358ad192ad246fde93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5f0d33ad8e144d8927f365c3d954738": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1efd98b4a4c5485181dd594af8a4d6b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c85d3eeb3db04eaf818b513cfb626de1",
              "IPY_MODEL_130a5564821743668c97227e267b1cdd",
              "IPY_MODEL_fb1ef171dae74ed9aada80cabe9f5c57"
            ],
            "layout": "IPY_MODEL_a79d24b9370e442d8575ac3cad48d3a5"
          }
        },
        "c85d3eeb3db04eaf818b513cfb626de1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5085c44f28ee494b813d265022491dba",
            "placeholder": "​",
            "style": "IPY_MODEL_97490bd9e28641bf93596668a48387f9",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "130a5564821743668c97227e267b1cdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f39a6a9f0e54d2f99441833e7ba9933",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_63541f66f28540ec91864cdad1c43ba9",
            "value": 2
          }
        },
        "fb1ef171dae74ed9aada80cabe9f5c57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a684657b6c934805a7409c0f6bd7079c",
            "placeholder": "​",
            "style": "IPY_MODEL_19b6b36688ba4e7087af48faf5b8bbba",
            "value": " 2/2 [00:59&lt;00:00, 27.24s/it]"
          }
        },
        "a79d24b9370e442d8575ac3cad48d3a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5085c44f28ee494b813d265022491dba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97490bd9e28641bf93596668a48387f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f39a6a9f0e54d2f99441833e7ba9933": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63541f66f28540ec91864cdad1c43ba9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a684657b6c934805a7409c0f6bd7079c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19b6b36688ba4e7087af48faf5b8bbba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a57486967d55491483b331468788d280": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_433ae3bb5f674e90a8971e9aacefa1e1",
              "IPY_MODEL_2874955e26804dae951ab4aba5de0e02",
              "IPY_MODEL_1e13ebe2ae184505bbcf692cf5c72975"
            ],
            "layout": "IPY_MODEL_a2d2261245c4497db660f4207c777780"
          }
        },
        "433ae3bb5f674e90a8971e9aacefa1e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8bcae2ac8143425c8df9775d7db8d6da",
            "placeholder": "​",
            "style": "IPY_MODEL_cc1e9884a6d14cc692ed7a482fc8b818",
            "value": "generation_config.json: 100%"
          }
        },
        "2874955e26804dae951ab4aba5de0e02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81d6094480454c09808a067c252f7948",
            "max": 188,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b82fe2c6c134329be35a0d7048cad94",
            "value": 188
          }
        },
        "1e13ebe2ae184505bbcf692cf5c72975": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eebbe7a3129048ccb5a4549024ff5176",
            "placeholder": "​",
            "style": "IPY_MODEL_38740f0a737f41a0893656efe1faa4e4",
            "value": " 188/188 [00:00&lt;00:00, 11.0kB/s]"
          }
        },
        "a2d2261245c4497db660f4207c777780": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bcae2ac8143425c8df9775d7db8d6da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc1e9884a6d14cc692ed7a482fc8b818": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81d6094480454c09808a067c252f7948": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b82fe2c6c134329be35a0d7048cad94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eebbe7a3129048ccb5a4549024ff5176": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38740f0a737f41a0893656efe1faa4e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5fcd892370ce45eb902f8b44a944a2d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_04f8611d8a134687bae4ee9f47eca01d",
              "IPY_MODEL_1c54fa0bf4614614b15b2cfe4ce459ac",
              "IPY_MODEL_d3a671e6d99043558d280e99b187c626"
            ],
            "layout": "IPY_MODEL_01df4917a7d24b4a859f74e041c86bbe"
          }
        },
        "04f8611d8a134687bae4ee9f47eca01d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05b6ec31702c46929a94c85e647b33a0",
            "placeholder": "​",
            "style": "IPY_MODEL_6b027178c1bf46439195e38eec576781",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "1c54fa0bf4614614b15b2cfe4ce459ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cd6061ba08d489d928401827c67768b",
            "max": 1618,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e6b8d546521542bfa76be537250fa227",
            "value": 1618
          }
        },
        "d3a671e6d99043558d280e99b187c626": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d768ca2f76564971b21ef7538e410da8",
            "placeholder": "​",
            "style": "IPY_MODEL_49d45175ff904a0c8cb0e0803e050bcb",
            "value": " 1.62k/1.62k [00:00&lt;00:00, 92.2kB/s]"
          }
        },
        "01df4917a7d24b4a859f74e041c86bbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05b6ec31702c46929a94c85e647b33a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b027178c1bf46439195e38eec576781": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9cd6061ba08d489d928401827c67768b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6b8d546521542bfa76be537250fa227": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d768ca2f76564971b21ef7538e410da8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49d45175ff904a0c8cb0e0803e050bcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0206ee5e2994028bb669e69d2ba7dcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cd7c13a2cf0946a19c7531eda2eef7c3",
              "IPY_MODEL_3686dc6780a749e29021efffa8ba7cd5",
              "IPY_MODEL_248451229550494b8a91b89b8dc264e7"
            ],
            "layout": "IPY_MODEL_03b16241c3114e6a9256a4a29b255c80"
          }
        },
        "cd7c13a2cf0946a19c7531eda2eef7c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_807f1ffb60d44f9790cac8f5411f8c3e",
            "placeholder": "​",
            "style": "IPY_MODEL_ccc8f4193f564964b92cb39ac8edee3e",
            "value": "tokenizer.model: 100%"
          }
        },
        "3686dc6780a749e29021efffa8ba7cd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b09d3309c5e4c499b1bbb42c65635e9",
            "max": 499723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7893ef8d7efd4d279928e4183c975b91",
            "value": 499723
          }
        },
        "248451229550494b8a91b89b8dc264e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2d7dd1834b34a40b13ba1f3c6601cdf",
            "placeholder": "​",
            "style": "IPY_MODEL_89ebfe22a13a46a79923da5141d60c66",
            "value": " 500k/500k [00:00&lt;00:00, 30.5MB/s]"
          }
        },
        "03b16241c3114e6a9256a4a29b255c80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "807f1ffb60d44f9790cac8f5411f8c3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccc8f4193f564964b92cb39ac8edee3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b09d3309c5e4c499b1bbb42c65635e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7893ef8d7efd4d279928e4183c975b91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e2d7dd1834b34a40b13ba1f3c6601cdf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89ebfe22a13a46a79923da5141d60c66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be1effc4e5934bc0a661da852f2e779b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_964ae5cfd742416fa6c05428da6bf9e4",
              "IPY_MODEL_b2e7c8572bd843f593580a046b019779",
              "IPY_MODEL_b629ec8f234c4dc39b31f43f026f4e75"
            ],
            "layout": "IPY_MODEL_382b53f506d64503967ad73b03d7503d"
          }
        },
        "964ae5cfd742416fa6c05428da6bf9e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd4253b8013248ec95b30791acf1ad14",
            "placeholder": "​",
            "style": "IPY_MODEL_6d448e67d9ff45c0ba19ef0606715081",
            "value": "tokenizer.json: 100%"
          }
        },
        "b2e7c8572bd843f593580a046b019779": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f4ec12e05494bafa4c045650940dbb4",
            "max": 1842767,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_968616924fba4ca5bf0fd84e4b201180",
            "value": 1842767
          }
        },
        "b629ec8f234c4dc39b31f43f026f4e75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdeac5a92d344864b4c10d4f674de087",
            "placeholder": "​",
            "style": "IPY_MODEL_6f461fb8049c402d8ac27e0280408f62",
            "value": " 1.84M/1.84M [00:00&lt;00:00, 14.7MB/s]"
          }
        },
        "382b53f506d64503967ad73b03d7503d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd4253b8013248ec95b30791acf1ad14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d448e67d9ff45c0ba19ef0606715081": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f4ec12e05494bafa4c045650940dbb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "968616924fba4ca5bf0fd84e4b201180": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fdeac5a92d344864b4c10d4f674de087": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f461fb8049c402d8ac27e0280408f62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "441912bc03d643339a9970135419585d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_979e84405c334ab9bf8475f9b61cf0ee",
              "IPY_MODEL_22ad924f3d0c48619ee34a70c954d3b0",
              "IPY_MODEL_9c56397dc6c6444fbdf1793cde031af6"
            ],
            "layout": "IPY_MODEL_71c94939266c45039d28efdf5eab08cf"
          }
        },
        "979e84405c334ab9bf8475f9b61cf0ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0784f293c77247ee9f3ea85a991600ad",
            "placeholder": "​",
            "style": "IPY_MODEL_feea17c4e542407187c367550610b839",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "22ad924f3d0c48619ee34a70c954d3b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2d43bb4f34c49a0a2a17fbf5040669c",
            "max": 414,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_16e260e165344c98a524e436f9a4e6e1",
            "value": 414
          }
        },
        "9c56397dc6c6444fbdf1793cde031af6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b94e6c769424a678902ad21d713a349",
            "placeholder": "​",
            "style": "IPY_MODEL_cfd28511004a4ee7b5e32dcbf5f5dc67",
            "value": " 414/414 [00:00&lt;00:00, 22.6kB/s]"
          }
        },
        "71c94939266c45039d28efdf5eab08cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0784f293c77247ee9f3ea85a991600ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "feea17c4e542407187c367550610b839": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2d43bb4f34c49a0a2a17fbf5040669c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16e260e165344c98a524e436f9a4e6e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0b94e6c769424a678902ad21d713a349": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfd28511004a4ee7b5e32dcbf5f5dc67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RAG (Retrieval Augmented Generation) System Using Llama2 With Hugging Face"
      ],
      "metadata": {
        "id": "lro9nTZdP2K4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CanrMWVrNc06",
        "outputId": "b3d41149-1a9e-4844-bc7e-f5f4673fd2bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-4.0.1-py3-none-any.whl (283 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/284.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m276.5/284.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.0/284.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-4.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- pypdf is used for uploading all the pdf docs."
      ],
      "metadata": {
        "id": "hhuFrDt7SXhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers einops accelerate langchain bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "II13yQ49ObCs",
        "outputId": "a980500b-3337-4119-d72b-f41c29fa1600"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.7/279.7 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m811.8/811.8 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.4/239.4 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- bitsandbytes is used in the quantization process means most of the LLM model will be in the 16 bits but since we are working on the Google Colab we will be quantized to 4 bits.\n",
        "- transformers will be used to create the pipeline which will be able to use Llama2 model with respect to any kind of input that we will specifically gives.\n",
        "- accelerate will speed in the process of uploading or loading the entire pipeline itself."
      ],
      "metadata": {
        "id": "PaNAGSY8Q5vX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding:\n",
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVceKJVzQw_2",
        "outputId": "87faa64c-403c-4332-e9b8-bf34795f15fc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence_transformers\n",
            "  Downloading sentence_transformers-2.3.1-py3-none-any.whl (132 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/132.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.8/132.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Installing collected packages: sentence_transformers\n",
            "Successfully installed sentence_transformers-2.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oe5ZNdkYSwjj",
        "outputId": "0cb72e07-5cf4-4fa3-e5c4-42853628f725"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama_index\n",
            "  Downloading llama_index-0.9.46-py3-none-any.whl (15.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama_index) (2.0.25)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama_index) (3.9.3)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.6.4)\n",
            "Collecting deprecated>=1.2.9.3 (from llama_index)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama_index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (2023.6.0)\n",
            "Collecting httpx (from llama_index)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama_index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (3.2.1)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama_index) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama_index) (1.23.5)\n",
            "Collecting openai>=1.1.0 (from llama_index)\n",
            "  Downloading openai-1.12.0-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama_index) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (8.2.3)\n",
            "Collecting tiktoken>=0.3.3 (from llama_index)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (4.9.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama_index) (0.9.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama_index) (4.0.3)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.9.3->llama_index) (1.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama_index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama_index) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama_index) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama_index) (4.66.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama_index) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama_index) (1.7.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama_index) (2.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->llama_index) (1.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama_index) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx->llama_index)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama_index) (3.6)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama_index)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama_index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama_index) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama_index) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama_index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama_index) (3.20.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama_index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama_index) (2023.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama_index) (1.2.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama_index) (23.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama_index) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama_index) (2.16.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama_index) (1.16.0)\n",
            "Installing collected packages: dirtyjson, h11, deprecated, tiktoken, httpcore, httpx, openai, llama_index\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed deprecated-1.2.14 dirtyjson-1.0.8 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 llama_index-0.9.46 openai-1.12.0 tiktoken-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing all the libraries\n",
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext # VectorStoreIndex is for indexing purpose, ServiceContext helps to combine llama2 model along with prompt template\n",
        "from llama_index.llms import HuggingFaceLLM # helps to interact with hugging face hub where you will be able to call the entire llama2 model\n",
        "from llama_index.prompts.prompts import SimpleInputPrompt"
      ],
      "metadata": {
        "id": "w07hxqUgTFaV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents=SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoL_zau7VFzD",
        "outputId": "cdf548c2-284b-4350-bec8-499fb5530101"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id_='1a702e93-bd70-4209-930e-1eef61213c75', embedding=None, metadata={'page_label': '1', 'file_name': 'Attention.pdf', 'file_path': '/content/data/Attention.pdf', 'file_type': 'application/pdf', 'file_size': 254261, 'creation_date': '2024-02-09', 'last_modified_date': '2024-02-09', 'last_accessed_date': '2024-02-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/274699544\\nAttention and Perception\\nChapt er · Mar ch 2015\\nDOI: 10.1002/9781118900772. etrds0018\\nCITATIONS\\n4READS\\n14,083\\n1 author:\\nRonald A R ensink\\nUniv ersity of British Columbia\\n141 PUBLICA TIONS \\xa0\\xa0\\xa012,021  CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll c ontent f ollo wing this p age was uplo aded b y Ronald A R ensink  on 26 F ebruar y 2018.\\nThe user has r equest ed enhanc ement of the do wnlo aded file.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='49cf63c4-6dc0-43df-888e-9c12cd8f6466', embedding=None, metadata={'page_label': '2', 'file_name': 'Attention.pdf', 'file_path': '/content/data/Attention.pdf', 'file_type': 'application/pdf', 'file_size': 254261, 'creation_date': '2024-02-09', 'last_modified_date': '2024-02-09', 'last_accessed_date': '2024-02-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='In Emerging Trends in the Social and Behavioral Sciences, R . A .  S c o t t  a n d  S . M .  K o s s l y n  ( E d s ) .   Thousand Oaks CA: Sage Publications.   Attention and Perception  Ronald A. Rensink Departments of Psychology and Computer Science University of British Columbia Vancouver, Canada    Abstract  This article discusses several key issues concerning the study of attention and its relation to visual perception, with an emphasis on behavioral and experiential aspects.  It begins with an overview of several classical works carried out in the latter half of the 20th century, such as the development of early filter and spotlight models of attention.  This is followed by a survey of subsequent research that extended or modified these results in significant ways.  It covers current work on various forms of induced blindness and on the capabilities of nonattentional processes.  It also includes proposals about how a \"just-in-time\" allocation of attention can create the impression that we see our surroundings in coherent detail everywhere, as well as how the failure of such allocation can result in various perceptual deficits.  The final section examines issues that have received little consideration to date, but may be important for new lines of research in the near future.  These include the prospects for a better characterization of attention, the possibility of more systematic explanations, factors that may significantly modulate attentional operation, and the possibility of several kinds of visual attention and visual experience. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='08780707-6cde-4ed1-834c-a51ea4f961c5', embedding=None, metadata={'page_label': '3', 'file_name': 'Attention.pdf', 'file_path': '/content/data/Attention.pdf', 'file_type': 'application/pdf', 'file_size': 254261, 'creation_date': '2024-02-09', 'last_modified_date': '2024-02-09', 'last_accessed_date': '2024-02-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='1. Introduction  Whenever we open our eyes, we experience an ever-changing world of colors, shapes, and movements.  This experience is so vivid and so compelling that we rarely stop to consider whether the underlying mechanisms may have limitations.  Instead, we simply have a strong impression that we always perceive everything in front of us.  Although we may need to scrutinize something on occasion, for the most part our visual system appears to operate in an automatic and seamless way, providing us with a complete and detailed representation of whatever is in our field of view.  But however appealing it may be, this impression cannot be correct.  Suppose someone wants to keep track of various players in a sports game.  A single player can usually be tracked without problem.  Three or four can also be tracked, although now with some effort.  But as the number increases further, simultaneous tracking of all the selected players becomes impossible.  Performance evidently depends upon a factor which enables certain kinds of perception to occur, but which has a clear limit to its capacity.  This factor is generally referred to as attention.  Work on human vision is providing increasing evidence that visual perception is the result of several interacting processes, most of which are quite sophisticated, and many of which have definite limits to their abilities.  And rather than the outputs of these processes accumulating in a detailed construction, much of our perception results instead from the co-ordination of these processes. In accord with this, much of our visual experience appears to depend on managing attention so that it is sent to the right item at the right time.  As such, attention is more than something that simply modifies or assists our perception on occasion—it is instead a factor central to our awareness of the world around us.  2. Foundational Research  It was recognized long ago that we need to pay attention to adequately perceive our surroundings.  But it is only recently that have we obtained a better understanding of what attention is and how it relates to perception.  Building upon proposals of philosophers of the 17th and 18th centuries, researchers in the 19th century began to map out several of its main characteristics.  For example, Hermann von Helmholtz discovered that an observer could attend ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ea48b3fd-1766-4795-9e6d-3bf23b545fdb', embedding=None, metadata={'page_label': '4', 'file_name': 'Attention.pdf', 'file_path': '/content/data/Attention.pdf', 'file_type': 'application/pdf', 'file_size': 254261, 'creation_date': '2024-02-09', 'last_modified_date': '2024-02-09', 'last_accessed_date': '2024-02-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='to (in the sense of recognizing) letters at locations outside of where the eyes were aimed (or \"fixated\"), showing that attention is not equivalent to eye fixation.  Meanwhile, William James distinguished \"sensorial\" from \"intellectual\" attention—the former concerned with concrete objects such as particular sports players, the latter with more abstract structures such as the quality of the game.  James also associated sensorial attention—in particular, visual attention, the focus of this review—with clarity of perception, intensity of perception, and visual memory.  Many of these concerns became an enduring backdrop for subsequent work. Filter Models  A more rigorous approach to understanding attention was developed during the middle decades of the 20th century, when researchers began focusing more on its selective aspects, and—in line with the \"cognitive revolution\" of that time—replaced the original emphasis on subjective experience with an emphasis on objective models.  Donald Broadbent proposed an influential filter model, in which conscious perception was achieved via a sequence of processes in a single perceptual pathway, with an attentional filter that gated selected aspects of a stimulus through to later processes.  An important issue was the locus of this filter: was it was early (selection affecting the initial stages, which measured simple properties such as color and motion) or late (selection appearing only at the highest levels, gating properties such as semantic category)?   The work undertaken to settle this issue resulted in a great deal of information about the ways various operations were affected by attention.  However, a complete resolution of this issue eluded researchers, and continues to do so to this day.  This strongly suggests that some of the original assumptions were incorrect: there may be, for example, more than one filter in the pathway (not to mention more than one pathway), making questions concerning a single filter somewhat ill-founded.  To get further insights, a different approach was needed. Spotlight Models  Despite the failure to determine whether selection was early or late, investigations into this issue resulted in a variety of new methodologies and new frameworks.  Over time, concerns about the nature of filters receded, and were replaced by an emphasis on how attention affected the representations themselves.   ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9d0b637c-9ca0-40a1-9121-92b59394da47', embedding=None, metadata={'page_label': '5', 'file_name': 'Attention.pdf', 'file_path': '/content/data/Attention.pdf', 'file_type': 'application/pdf', 'file_size': 254261, 'creation_date': '2024-02-09', 'last_modified_date': '2024-02-09', 'last_accessed_date': '2024-02-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' An example of such a methodology is visual search, where observers are asked to report on a prespecified target item in a visual display.  It was found, for example, that some items can be detected immediately and without much attention (e.g., a blue dot among a set of yellow dots) whereas others cannot (e.g., a \"T\" among a set of \"L\"s).  Among the more prominent frameworks to account for such findings was Anne Treisman\\'s feature integration theory.  This framework modeled visual processing in terms of two stages.  The first is a preattentive stage that determines simple properties (features) such as color or motion rapidly and in parallel at each point of the visual field, resulting in a \"map\" describing the spatial distribution of each feature.  The second involves a limited-capacity \"spotlight\" of attention that travels from item to item at a rate of about 50 milliseconds per item, not only filtering but also binding features that correspond to the same item (e.g., integrating the representations of the \"blue\" and \"vertical\" properties at a location into a single representation of both).  Later refinements included the guided search model of Jeremy Wolfe and colleagues, in which items in a feature map could be selectively inhibited or excited to improve the efficiency of search.  Other variants examined issues such as the extent to which attention might be allocated in parallel rather than in a serial fashion.  All these models had natural connections to other areas of vision science: for example, the features found in visual search could be related in a fairly straightforward way to many of the elements underlying texture perception.  Other approaches yielded similar results.  Michael Posner and colleagues did seminal work on cuing, showing that if a cue (such as a dot) were shown at the location of a target just before the target appeared, detection could be sped up by several hundred milliseconds.  This speedup diminishes as the separation between target and cue is increased, something readily accounted for by a model in which the edges of the spotlight are smooth.  Meanwhile, Charles Eriksen and colleagues showed that a spotlight mechanism could also account for the ability of nearby items (or flankers) to interfere with detection; results also suggested that only one spotlight operates at any time, and that it can rapidly adjust its size, \"zooming\" in or out as required by the task.  Owing to its ability to account for a variety of effects, therefore, the spotlight model has become the \"classical\" explanation of visual attention, forming the basis of much of our current understanding of how it operates. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4f4c1e04-fed5-4a8e-9bf8-2f4fb55c7d20', embedding=None, metadata={'page_label': '6', 'file_name': 'Attention.pdf', 'file_path': '/content/data/Attention.pdf', 'file_type': 'application/pdf', 'file_size': 254261, 'creation_date': '2024-02-09', 'last_modified_date': '2024-02-09', 'last_accessed_date': '2024-02-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Multiple-Object Tracking  A rather different approach to studying attention was developed by Zenon Pylyshyn and colleagues, based on multiple-object tracking.  Here, a set of identical items—dots on a screen, say—is initially displayed.  A subset of these is marked (e.g., some of the dots flash) and the marked items then tracked as they randomly move around the display.  The ability to track is severely limited: under most conditions no more than 3 or 4 can be handled.  The extent to which multiple-object tracking can be explained by a spotlight mechanism remains unclear.  However, there is considerable—although not universal—belief that this tracking does involve a form of attention, if only because of the limited capacity found. Underlying Mechanisms  One of the more successful quantitative models of attentional filtering and binding was the Theory of Visual Attention of Claus Bundesen, which could account for a considerable variety of experimental data.  It was also compatible with later suggestions that filtering and binding could be implemented via neural assemblies that inhibit their neighbors when activated.  Another (possibly complementary) proposal about implementation was neural synchrony, which posited that an attended item could be represented by the synchronized firing of a group of neurons.  More generally, many of the classical results could be explained by models based on the dynamics of neural interactions, along with the selective routing of information from various areas of the brain.  In parallel with this, other work focused on understanding attentional control.  Michael Posner suggested that the movement of attention involved three distinct components: (i) the disengagement of attention from the current item being attended, (ii) the shifting of its location (e.g., the center of the spotlight) over space, and (iii) the re-engagement of attention on a new item.  Among other things, this model successfully accounted for several perceptual problems encountered in developmental disorders and degenerative diseases.  Subsequent work placed an increased emphasis on the extent to which control was affected by properties of the image—for example, the extent to which the size or color of an item differed from that of its neighbors. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f4f73470-dcd0-4cd9-b088-c5e8ce5d1851', embedding=None, metadata={'page_label': '7', 'file_name': 'Attention.pdf', 'file_path': '/content/data/Attention.pdf', 'file_type': 'application/pdf', 'file_size': 254261, 'creation_date': '2024-02-09', 'last_modified_date': '2024-02-09', 'last_accessed_date': '2024-02-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3. Cutting-Edge Research  The late 20th and early 21st century saw the development of several new research directions.  Some were direct continuations of classical work, and led to further refinement of earlier results.  But others involved new perspectives, and sometimes caused a reconsideration of previous assumptions.  Although these investigations have yet to result in a coherent, generally-accepted account of attention, they have provided a better understanding of its operation, including how it relates to other mechanisms involved in visual perception, and how its limitations can intrude into everyday life. Induced Blindness  Much of recent work has returned to the issue of how attention relates to conscious visual experience—in particular, the way that an absence of attention can cause a failure to see an item in clear view of the observer.  One example is inattentional blindness, where an observer fails to see an unexpected object or event, even when these are large and quite visible.  This has been taken to indicate that attention is needed to see an object or event.  Some uncertainty exists as to the extent of its implications at the theoretical level: does the observer fail to see all aspects of the object, or do they still see its basic features but are blind to its structure or meaning?  Either way, inattentional blindness is increasingly recognized as being important at the practical level.  For example, many traffic accidents are likely due to a driver failing to see a pedestrian (or another car) because their attention was focused on something else.   A variant of this is continuous flash suppression.  Here, a set of random images is continually flashed into one eye at a rate of about 10 Hz, suppressing the experience of the image shown to the other eye.  This can be sustained for several minutes.  Various explanations have been put forward for this phenomenon.  The predominant hypothesis is that it occurs because attention cannot be sent to the suppressed image, and that no other effects are responsible—i.e., that continuous flash suppression is a form of inattentional blindness.  If so, it could be a powerful way to study the extent to which perception can occur in the absence of conscious visual experience.  Another phenomenon that has received a great deal of interest is change blindness.  Here, the observer fails to notice a change that occurs in an object, even if the change is large and can ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f04aa422-ca5e-4cef-be98-f3d391a11ba2', embedding=None, metadata={'page_label': '8', 'file_name': 'Attention.pdf', 'file_path': '/content/data/Attention.pdf', 'file_type': 'application/pdf', 'file_size': 254261, 'creation_date': '2024-02-09', 'last_modified_date': '2024-02-09', 'last_accessed_date': '2024-02-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='easily be seen once the observer knows what it is.  This phenomenon strongly suggests that attention is needed to see change.  It appears that attention engages visual short-term memory (vSTM) to create a representation that is coherent—i.e., is integrated over some extent of space and has continuity over some duration of time.  The number of items that can be monitored simultaneously for change is about 3 or 4, a limit similar to the capacity of vSTM.  Unlike inattentional blindness, change blindness can occur even when a change is expected.  This can lead to severe problems in everyday life, in that people can miss even a large, obvious event if they are not attending to it the moment it occurs.  Other types of induced blindness are also of interest.  One of these is the attentional blink.  This occurs when two different (prespecified) targets in a stream of rapidly-presented stimuli appear at slightly different times; under some conditions, the first target will be seen but not the second.  This has been explained in terms of attention not being allocated to the second item in time, possibly because the representation for the first has not yet been completed.  A related phenomenon is repetition blindness, where the observer can miss the occurrence of a repeated item in a stream of rapidly-presented images.  This is likewise believed to be due to the failure of attention to create sufficiently quickly a representation of the repeated item. Nonattentional Processing  The earliest stages of visual processing are generally thought to be concerned with simple properties such as color, motion, and orientation.  It was originally assumed that attention acted directly on such properties—that they were the preattentive features uncovered in visual search.  But later work showed that search can be influenced by relatively complex localized structures (or proto-objects) created by processes acting prior to attention.  These processes can group line segments, bind features, interpret dark regions as shadows, and perhaps even recover three-dimensional orientation at each location in the image, essentially creating a \"quick and dirty\" map of scene structure.  The strength of cuing and speed of search can be similarly influenced by the inferred structure of the background, being enhanced for items on the same surface and diminished for items on different ones.  All these results point to a considerable amount of processing that occurs rapidly (and likely in parallel across the visual field), before attention has had much of a chance to operate.  ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fd9ebfa5-991b-452a-9641-4a033ceabc7b', embedding=None, metadata={'page_label': '9', 'file_name': 'Attention.pdf', 'file_path': '/content/data/Attention.pdf', 'file_type': 'application/pdf', 'file_size': 254261, 'creation_date': '2024-02-09', 'last_modified_date': '2024-02-09', 'last_accessed_date': '2024-02-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=' Recent work has also shown that observers can accurately estimate summary statistics, such as the average size of the disks in an image, even if this image is presented for only a hundred milliseconds or so; more sophisticated properties (e.g., Pearson correlation) can also be estimated this way.  Observers can even determine the appropriate category (or gist) of a scene under such conditions, possibly based on these statistics.  In all of these, there is no time to filter or bind more than a few items, suggesting the existence of processes that operate prior to—or perhaps in tandem with—visual attention.    The \"intelligence\" of such nonattentional processes is an open issue.  Observers show little inattentional blindness to words and pictures with a strong emotional impact (e.g., the observer\\'s name), indicating that some degree of recognition exists before attention is sent to the item.  In general, then, all these results imply that nonattentional processes are capable of more than previously believed.  And attention may correspondingly do less: although attention can be used on occasion to bind visual features, for example, it may not be necessary for all aspects of binding.   Connections with Scene Perception  Phenomena such as inattentional blindness and change blindness suggest that attention is necessary for visual experience.  And most studies concur that attention is severely limited.  Why then do we not experience such limits when viewing a scene?  One possibility is that attention can create a representation—a visual object—possessing detail and coherence, but only as long as attention is maintained.  If this can be done on a \"just in time\" basis—i.e., attention is sent to the right item at the right time—the result would be a virtual representation that would appear to higher-level processes as if it were \"real\", i.e., as if it contained detailed and coherent representations everywhere.  An important goal of current work is therefore to understand the nature of the mechanisms underlying such co-ordination.  One suggestion begins with nonattentional processes providing a constantly-regenerating array of proto-objects, which represent simple properties of the scene that are visible to the observer.  Attention can select a subset of these, \"knitting\" them into a coherent visual object.  In tandem with this, the statistics of the (unattended) proto-object array could determine gist; this could help access high-level knowledge about the scene, and so guide attention to appropriate parts of the image.  In this characterization, then, scene representations are no longer long-lasting ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='16e615cf-40e1-43e6-8b9d-5f7e23934cd7', embedding=None, metadata={'page_label': '10', 'file_name': 'Attention.pdf', 'file_path': '/content/data/Attention.pdf', 'file_type': 'application/pdf', 'file_size': 254261, 'creation_date': '2024-02-09', 'last_modified_date': '2024-02-09', 'last_accessed_date': '2024-02-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='structures built up from eye movements and attentional shifts, but are relatively temporary structures that guide such activities.  Among other things, this implies that different observers—with different knowledge, different goals, and therefore different attentional strategies—can literally see the same scene differently. Connections with Perceptual Deficits  Given that attention is needed for visual experience, problems with its allocation may explain various perceptual deficits.  In unilateral neglect, for example, patients with damage to the right posterior parietal cortex (at the top and back of the head) can fail to visually experience whatever is in the left half of the visual field, even if this is directly in front of them.  (Oddly, a corresponding deficit does not usually result from damage to the left side.)  A related condition is extinction, where such a failure also occurs, but only when an something also exists in the right half of the visual field.  Such deficits may result from problems in shifting attention to the relevant location (or at least, keeping it there), possibly because of damage to the parietal circuits that control it.  Interestingly, words and pictures in the neglected—and presumably unattended—part of the visual field can still affect the observer, consistent with the proposal of intelligent nonattentional processes.  Another condition likely related to these is simultanagnosia.  Patients with this deficit cannot see more than one coherent object (or coherent part of an object) at a time; the rest of the scene is experienced only in a fragmented way, or not experienced at all.  This has been associated with damage to the parieto-occipital areas (at the upper part of the back of the head), which may cause problems in allocating attention to particular objects.    4. Key Issues for Future Research   Most issues in attention research—both classical and subsequent—are still far from being resolved.  For example, what is the relation between attention and vSTM?  How many nonattentional process exist, and how intelligent is each?  How exactly do the knowledge and goals of the observer determine how attention is allocated?  The answers to all of these are ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='36543625-1522-451e-ac88-36e1bede69db', embedding=None, metadata={'page_label': '11', 'file_name': 'Attention.pdf', 'file_path': '/content/data/Attention.pdf', 'file_type': 'application/pdf', 'file_size': 254261, 'creation_date': '2024-02-09', 'last_modified_date': '2024-02-09', 'last_accessed_date': '2024-02-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='necessary for a complete understanding of attention.  Finding them will take many more years of work.  Meanwhile, other issues are also beginning to emerge.  Part of the reason they have received little consideration to date is sociological: given the work still to be done on current issues, little incentive exists to embark upon riskier ventures elsewhere.  Part is methodological: it is unclear how some of these issues could be addressed in a productive way.  And part is simple ignorance: we didn\\'t know enough until recently to realize that some of these issues even existed.  But whatever the reason for their previous obscurity, many of these issues are becoming increasing prominent, and may well form a critical part of future research. Characterization  One of the most basic—and oldest—issues concerning attention concerns its nature: what exactly is it?  Over the years, attention has been characterized in various ways, such as the quality of visual experience, or a l i m i t e d “resource” that enables particular operations to be carried out.  But the greatest increase in our understanding seems to have been achieved by focusing on the idea of selection.  Could this idea be developed further, ideally in a way consistent with most of the other characterizations that have been applied?  One possibility would be to define an attentional process as one that is contingently selective, with that selectivity controlled via global considerations (e.g., tracking a particular person of interest).  From this perspective, “attention” is more an adjective than a noun.  Any globally-controlled process of limited capacity—such as binding visual features, or placing them into vSTM—would be \"attentional\", since limited capacity implies selectivity of one form or other.  This would also be the case for any process that selectively improves the quality of visual experience, provided only that this is done on the basis of some global consideration (e.g., not done reflexively). Computational Explanation  Even if attention could be described in terms of a particular function or mechanism, our understanding of it would be incomplete: we might know how it operates, but not why.  For example, if some capacity were limited to three items, why should this be?  Why not four?  Why ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='350bb0a3-ecec-443a-946f-96cf3b8c8c65', embedding=None, metadata={'page_label': '12', 'file_name': 'Attention.pdf', 'file_path': '/content/data/Attention.pdf', 'file_type': 'application/pdf', 'file_size': 254261, 'creation_date': '2024-02-09', 'last_modified_date': '2024-02-09', 'last_accessed_date': '2024-02-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='not one?  Of course, such a limit may simply be an accident of history.  But it may also reflect the influence of deeper principles.  One possible way of investigating this is to apply the computational framework of David Marr.  This framework posits that any (visual) process can be analyzed from three interlocking perspectives: (i) function (both description and justification), (ii) mechanism (algorithm and representation), and (iii) neural implementation.  Such explanations have led to deep insights into the nature of processes at early levels of human vision, and have helped develop their equivalents in machine vision.  A few studies, such as those of John Tsotsos, have begun applying this approach to attention as well.  Such analyses could eventually provide considerable insights into its nature and the exact role it plays in perception. Modulatory Factors  Attention is often assumed to be governed entirely by the demands of the task and the knowledge of the observer.  However, evidence is emerging that other factors also play an important role: 1. Stress.  Stress can cause tunneling, where the observer loses awareness of anything beyond the center of their visual field.  It can also speed up visual search for simple features (e.g., a particular orientation, such as \"vertical\"), although apparently not for their combination (e.g., \"blue\" and \"vertical\").  Such effects suggest that stress causes attention to improve its selectivity by reducing the range of the properties allowed through.  However, it may be that such improvement is obtained at the cost of a slower switching of the underlying mechanisms. 2. Aging.  Another important perspective is how attention changes over lifespan.  Different aspects of attention appear to be differently affected: filtering and binding appear to be largely unaffected, while top-down control (e.g., disregard of irrelevant stimuli, switching speed) deteriorates noticeably with age.  More investigation would be of great practical importance, and could provide new perspectives on underlying mechanisms. 3. Cultural / Visual Environment.  Recent work suggests that observers from Western countries (e.g., the United States) generally attend to individual objects in a scene, whereas observers from East Asian countries (e.g., Japan) generally attend to the scene as ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1445ed8f-f4da-4e42-8fe0-a4540575dcb2', embedding=None, metadata={'page_label': '13', 'file_name': 'Attention.pdf', 'file_path': '/content/data/Attention.pdf', 'file_type': 'application/pdf', 'file_size': 254261, 'creation_date': '2024-02-09', 'last_modified_date': '2024-02-09', 'last_accessed_date': '2024-02-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='a whole.  Western observers show a search asymmetry: they can detect a long line among short lines more quickly than vice versa.   Meanwhile, East Asian observers are equally slow for both.  Preliminary work suggests that some of these differences disappear when significant time is spent in the other culture.  If these results hold, they would indicate a strong effect of culture—or at least, visual environment—on the way attention is used.  Interesting issues would then arise as to which (visual) characteristics are relevant, and why. 4. Mental Set.  Attentional control—including the speed of visual search—can be influenced by the nature of the task and explicit instruction to the observer.  Such results suggest that an observer may have available several processing modes, each corresponding to a particular \"mental set\".  (Some of these may account for the cultural differences mentioned above.)  If so, interesting questions arise as to the nature of these modes, and the conditions that trigger them. Kinds of Attention  Another important issue is whether there exists one kind of attention or several.  Occasional conflicts have occurred in claims regarding the speed, sensitivity, and even function of attention.  Some of these issues could be resolved if there existed more than one kind of attention.  The existence of different kinds of attention would also create new challenges, such as determining the taxonomy that would best describe these kinds, and establishing the various ways in which a process could be \"preattentive\" or \"nonattentional\".  Based on function, speed, and structures operated upon, several groupings of attentional processes can be delineated.  An important question is the extent to which these groupings correspond to distinct aspects—or even kinds—of attention (or perhaps more precisely, attentional processing): 1. Attentional Sampling.  This is the selective pickup of information by the eye.  The eye has high acuity and color perception only in the few degrees around the point of fixation.  It must therefore—together with the head and body—move around to pick up the right information from the environment.  Sampling has traditionally been referred to as overt ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='eb432f71-64a5-4cfa-829d-d790f6655c7c', embedding=None, metadata={'page_label': '14', 'file_name': 'Attention.pdf', 'file_path': '/content/data/Attention.pdf', 'file_type': 'application/pdf', 'file_size': 254261, 'creation_date': '2024-02-09', 'last_modified_date': '2024-02-09', 'last_accessed_date': '2024-02-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='attention.  It has long been known to differ from operations carried out internally, which are often collectively referred to as covert attention. 2. Attentional Filtering (Gating).  Irrelevant information can degrade performance, and must be removed as soon as possible.  Ways of doing so include spatial filtering (selection only from a particular region of space) and feature filtering (selection of items containing a particular feature); these are largely the focus of classical approaches.  Selection can be diffuse (over a wide range) or focused (over a restricted range).  It appears that the mechanisms involved can be switched quickly (typically, within 50 milliseconds) and operate on the basis of simple properties, such as color, motion, or spatial position. 3. Attentional Binding.  This is the selective linking of properties so as to capture the structure of the world at any given moment.  This can be done in various ways, such as feature binding (e.g., linking the color and orientation of an item) and position binding (e.g., linking an item to a precise position in space).  Binding differs from filtering, being concerned not with access, but construction.  The mechanisms involved also appear to differ, being slower (completing within about 150 milliseconds) and involving organized structures rather than simple properties. 4. Attentional Holding.  When a physical object changes over time (e.g., a bird takes flight), it is useful to perceive an underlying structure that remains the same.  The associated representation must be stabilized (or \"held\") across time, likely via vSTM; such holding therefore differs from binding.  The mechanisms involved also appear to differ, being even slower (completing within about 300 milliseconds) and operating on no more than 3-4 items at a time. 5. Attentional Individuating.  It is often useful to perceive not just an object, but a particular object (e.g., when determining if one item is to the left of another).  Such individuating (or \"indexing\") may also be the basis of tracking.  The mechanisms involved can act quickly (about 50 milliseconds per item) and involve up to 7-8 structures at a time. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e69a8abf-bf1c-4704-926c-9b0ac9a59611', embedding=None, metadata={'page_label': '15', 'file_name': 'Attention.pdf', 'file_path': '/content/data/Attention.pdf', 'file_type': 'application/pdf', 'file_size': 254261, 'creation_date': '2024-02-09', 'last_modified_date': '2024-02-09', 'last_accessed_date': '2024-02-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Kinds of Visual Experience   A parallel set of concerns involves the nature of conscious visual experience.  As in the case of attention, it has been widely assumed there exists only one kind of visual experience.  But just as color and motion are distinct aspects—or even kinds—of experience concerned with distinct physical properties of the world, so might there be other kinds of experience concerned with distinct structural properties: 1. Fragmented experience.  This is the experience of simple features with little structure and poor localization; in some ways, it is what is experienced when viewing an Impressionist painting.  It can be encountered in brief displays, where the experience is one of a fleeting array of simple colors and poorly-articulated shapes.  This has sometimes been referred to as \"background consciousness\"—the experience of the background when attention (binding) is focused on foreground objects. 2. Assembled experience.  This is the experience of unstructured properties (fragmented experience) along with a degree of superimposed static structure.  It can be encountered in displays presented for about 150 milliseconds, the time needed for binding; it is essentially what is experienced under stroboscopic conditions.  Although no new sensory (physical) properties are present, new kinds of structure are (e.g., the linking of line segments into particular shapes).  Among other things, this distinction allows two kinds of inattentional blindness to be distinguished:  Type 1, the absence of fragmented experience (i.e., the absence of sensory qualities, perhaps caused by an absence of attentional gating), and Type 2, the absence of assembled experience, with simple sensory qualities still present but no higher-level structure (perhaps caused by an absence of attentional binding). 3. Coherent experience.  This is the \"standard\" experience encountered when giving complete attention to a physical object: not only is the static structure of assembled experience present, but also movement—or more generally, change—along with the impression of an underlying substrate that persists over time.  The absence of coherent experience (change blindness) might be regarded as Type 3 inattentional blindness, caused by an absence of attentional holding. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4a35ed44-b467-41a5-a5b6-34f06abd3ae9', embedding=None, metadata={'page_label': '16', 'file_name': 'Attention.pdf', 'file_path': '/content/data/Attention.pdf', 'file_type': 'application/pdf', 'file_size': 254261, 'creation_date': '2024-02-09', 'last_modified_date': '2024-02-09', 'last_accessed_date': '2024-02-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4. Sensing.  Observers in change detection experiments occasionally report that they “sense” or “feel” a change without having any visual experience of it.  The status of this \"sensing\" is controversial.  It is sometimes considered simply a \"weakened\" form of seeing (i.e., coherent experience).  However, it differs qualitatively from the other kinds of visual experience, and appears to involve different mechanisms as well. An important challenge for future work is to determine the extent to which these really are distinct kinds of visual experience, and how they may relate to various kinds of attention. Important issues also exist concerning what might be called \"dark structure\": structure that is never experienced, yet still affects visual perception.  5. Conclusion  The nature of attention and its relation to perception have long been issues cloaked in mystery, involving matters that are highly subjective and poorly defined.  But a great deal of progress has been made, particularly over the past century.  A considerable amount of understanding now exists as to how attention operates, and the role it plays in our conscious experience.  And, importantly, this understanding has suggested new questions, concerning issues that researchers of earlier times had not even imagined.  Investigating these issues will no doubt require much time and effort.  But the results are likely to shed interesting new light on the way we experience our world. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='673c7af0-4dfb-483e-8cec-4a73216543f0', embedding=None, metadata={'page_label': '17', 'file_name': 'Attention.pdf', 'file_path': '/content/data/Attention.pdf', 'file_type': 'application/pdf', 'file_size': 254261, 'creation_date': '2024-02-09', 'last_modified_date': '2024-02-09', 'last_accessed_date': '2024-02-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Further Reading Bundesen, C. & Habekost, T.  (2008).  Principles of Visual Attention: Linking Mind and Brain.  Oxford: University Press. Itti, L., Rees, G., & Tsotsos, J.K. (2005).  The Neurobiology of Attention.  San Diego: Academic Press. Mack, A., & Rock, I. (1998).  Inattentional Blindness.  Cambridge MA: MIT Press. Pashler, H.E. (1999).  The Psychology of Attention.  Cambridge MA: MIT Press. Rensink, R.A. (2013). Perception and attention.  In D. Reisberg (Ed). Oxford Handbook of Cognitive Psychology.  Oxford: University Press.  pp. 97-116. Simons, D.J. (Ed.). (2000).  Change Blindness and Visual Memory.  New York: Psychology Press.  Styles, E.A.  (2006).  The Psychology of Attention (2nd ed.).  New York: Psychology Press. Tsotsos, J.K.  (2011).  A Computational Perspective on Visual Attention. Cambridge MA: MIT Press. Wolfe J.M. (2000).  Visual attention.  In K.K. De Valois (Ed), Seeing (2nd ed).  San Diego: Academic Press.  pp. 335-386. Wright, R.D. (Ed.).  (1998). Visual Attention.  Oxford: University Press. ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f5d48942-7fab-449f-a7f5-dfe77a8272ce', embedding=None, metadata={'page_label': '18', 'file_name': 'Attention.pdf', 'file_path': '/content/data/Attention.pdf', 'file_type': 'application/pdf', 'file_size': 254261, 'creation_date': '2024-02-09', 'last_modified_date': '2024-02-09', 'last_accessed_date': '2024-02-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Biography  Ronald A. Rensink is an Associate Professor in the departments of Computer Science and Psychology at the University of British Columbia (UBC) in Vancouver, Canada.  His interests include human vision (particularly visual attention and consciousness), computer vision, visual design, and the perceptual mechanisms used in visual analysis.  He obtained a PhD in Computer Science from UBC in 1992, followed by a postdoctoral fellowship for two years in the Psychology department at Harvard University.  This was followed by six years as a research scientist at Cambridge Basic Research, a laboratory sponsored by The Nissan Motor Company.  He returned to UBC in 2000.  He is currently part of the UBC Cognitive Systems Program, an interdisciplinary program that combines Computer Science, Linguistics, Philosophy, and Psychology.  Among other things, he is a co-founder of the Vancouver Institute for Visual Analytics (VIVA), an institute dedicated to facilitating the development of systems that can combine human and machine intelligence in optimal ways.  Webpage:     http://www.psych.ubc.ca/~rensink   http://www.cs.ubc.ca/~rensink \\nView publication stats', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9704f5fd-0564-4cd0-92b0-969b36168cef', embedding=None, metadata={'page_label': '1', 'file_name': 'Yolo.pdf', 'file_path': '/content/data/Yolo.pdf', 'file_type': 'application/pdf', 'file_size': 427716, 'creation_date': '2024-02-09', 'last_modified_date': '2024-02-09', 'last_accessed_date': '2024-02-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/351411017\\nReal-T ime Object Detection Using YOLO: A Review\\nPreprint  · May 2021\\nDOI: 10.13140/RG.2.2.24367.66723\\nCITATIONS\\n15READS\\n15,774\\n2 author s:\\nUpulie Handalag e\\nUniv ersität des Saarlandes\\n5 PUBLICA TIONS \\xa0\\xa0\\xa017 CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nLakshini K uganandamurthy\\nSri Lank a Instit ute of Inf ormation T echnolog y\\n3 PUBLICA TIONS \\xa0\\xa0\\xa014 CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll c ontent f ollo wing this p age was uplo aded b y Upulie Handalag e on 08 May 2021.\\nThe user has r equest ed enhanc ement of the do wnlo aded file.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='29dc3f44-650c-45fc-b6dd-d04a045829d2', embedding=None, metadata={'page_label': '2', 'file_name': 'Yolo.pdf', 'file_path': '/content/data/Yolo.pdf', 'file_type': 'application/pdf', 'file_size': 427716, 'creation_date': '2024-02-09', 'last_modified_date': '2024-02-09', 'last_accessed_date': '2024-02-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Real-Time Object Detection using YOLO: A review  \\nUpulie H.D.I  \\nIT18107074  \\nSri Lanka Institute of Information Technology  \\nMalabe, Sri Lanka  \\nireshaupulie@gmail.com   \\nLakshini Kuganandamurthy  \\nIT17073592   \\nSri Lanka Institute of Information Technology  \\nMalabe, Sri Lanka  \\nlakkuga@gmail.com  \\n \\nAbstract—With the availability of eno rmous amounts of data \\nand the need to computerize visual -based systems, research on \\nobject detection has been the focus for the past decade. This need \\nhas been accelerated with the increasing computational power \\nand Convolutional Neural Network (CNN) advan cements since \\n2012. With various CNN network architectures available, the \\nYou Only Look Once (YOLO) network is popular due to its \\nmany reasons, mainly its speed of identification applicable in \\nreal-time object identification. Followed by a general \\nintroduc tion of the background and CNN, this paper wishes to \\nreview the innovative, yet comparatively simple approach \\nYOLO takes at object detection.  \\nKeywords —YOLO, CNN, object detection, image classification  \\n \\nI. INTRODUCTION  \\nAlthough the human eye is capable of instantly and \\nprecisely identifying a given visual, including its content, \\nlocation, and visuals close by interacting with it, the human \\nmade, computer vision -enabled systems are relatively low in \\naccuracy and speed. Any advancements leading to \\nimprovement s in efficiency and performance in this field \\ncould pave paths to creating more intelligent systems, much \\nlike humans. These advancements, in turn, would ease human \\nlife through systems such as assistive technologies that allow \\nhumans to complete tasks wit h little to no conscious thought. \\nFor instance, driving a car equipped with a computer vision -\\nenabled assistive technology could predict and notify a driving \\ncrash prior to the incident, even if the driver is not conscious \\nof their actions. Therefore, real -time object detection has \\nbecome a highly required subject in continuing the automation \\nor replacement of human tasks. Computer vision and object \\ndetection are prominent field s under machine learning and are \\neventually expected to aid unlocking the potential general -\\nresponsive robotic systems.  \\nWith the current technological advancements, creating \\nopenness and attainability of data to and from everyone \\nconnected to it has become  an easy task. Most human lives \\nrevolved around mainstream personal computers (PCs), and \\nsmartphones have made this process even more accessible. \\nAlong with this process, the expansion of information and \\nimages available on the internet/cloud has become to  the point \\nof millions per day. Usage of computerized systems to utilize \\nthis information and make necessary recognitions and \\nprocesses is vital due to humans' impracticality performing the \\nsame iterative tasks. The initial step of most such processes \\nmay include recognizing a specific object or area on an image. \\nDue to the unpredictability of the availability, location, size, \\nor shape of an item in each image, the recognition process is \\ninconceivably hard to be performed through a traditional \\nprogrammed co mputer algorithm. Factors such as the \\ncomplexity of the foundation, light intensities too contribute \\nto this.  Different strategies have been proposed to solve the \\nproblem of object identification throughout the years. These \\ntechniques focus on the solutio n through multiple stages. \\nNamely, these core stages include recognition, classification, \\nlocalization, and object detection. Along with the \\ntechnological progression over the years, these techniques \\nhave been facing challenges such as output accuracy, res ource \\ncost, processing speed and complexity issues. With the \\ninvention of the first Convolutional Neural Network (CNN) \\nalgorithm in the 1990s inspired by the Neocognitron by Yann \\nLeCun et al. [1] and significant inventions like AlexNet [2], \\nwhich won the ImageNet Large Scale Visual Recognition \\nChallenge (ILSVRC) in 2012 (thus later referred to as \\nImageNe t) CNN algorithms have been capable of providing \\nsolutions for the object detection problem in various \\napproaches. With the purpose of improving accuracy and \\nspeed of recognition, optimization focused algorithms such as \\nVGGNet [3], GoogLeNet [4] and Deep Residual Learning \\n(ResNet) [5] have been invented over the years.  \\nAlthough these algorithms improved over time, window \\nselection or identifying multiple objects from a single image \\nwas still an issue. To bring solu tions to this issue, algorithms \\nwith region proposals, crop/warp features, SVM \\nclassifications and bounding box regression such as Regions \\nwith CNN (R -CNN) were introduced. Although R -CNN was \\ncomparatively high in accuracy with the previous inventions, \\nits high usage of space and time later led to  the invention of  \\nSpatial Pyramid Pooling Network (SPPNet) [6]. Despite \\nSPPNet's speed , to reduce the similar drawbacks it shared with \\nR-CNN; Fast R -CNN was introduced. Although Fast R -CNN \\ncould reach real -time speeds using very deep networks, it held \\na computational bottleneck. Later Faster R -CNN, an \\nalgorithm based on ResNet, was introduced. Due to Faster R -\\nCNN not yet capable of surpassing state of the art detection \\nsystems, YOLO was introduced. This paper reviews the \\ndominating real -time object detection algorithm You Only \\nLook Once (YOLO).  \\nConsisting of layers in the basic CNN architecture and \\nYOLO network s, each layer's characteristics and t he two \\nversions of YOLO; YOLO -V1 and YOLO -V2 would be \\nreviewed under this paper. The strengths and weaknesses of \\nYOLO would be exposed, finally being fo llowed by a \\nsummarized conclusion.  \\n \\nII. CONVOLUTIONAL NEURAL NETWORK (CNN)  \\nA Convolutional Neural Network (CNN) could be taken \\nas a subcategory under Deep Neural Networks specifically \\ninvented for image processing and object detection. CNN \\nalgorithms can be ut ilized without requiring an enormous \\namount of predefined substantial parameters for the provided \\nimage. This ease at training a model and the vast amount of \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1cdb17fc-b593-403a-9b00-20d764ae4e3f', embedding=None, metadata={'page_label': '3', 'file_name': 'Yolo.pdf', 'file_path': '/content/data/Yolo.pdf', 'file_type': 'application/pdf', 'file_size': 427716, 'creation_date': '2024-02-09', 'last_modified_date': '2024-02-09', 'last_accessed_date': '2024-02-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"information available through the internet has made CNN \\nalgorithms possible. The mechanism CNN alg orithms follow \\nto express and extract features of the input data is entirely \\nmathematical. This mechanism involves a weight sharing \\nprocess that recognizes and identifies information that holds \\nsimilar features. This process enables networks to analyze \\nhigh data dimensions to achieve the final output of excellent \\nclassification in the end. One of the apparent obstacles in \\nmoving forward with getting better results using CNN \\nmodels is the processing capabilities of available hardware \\nand the scope of paramet ers in datasets.  \\nThe invention of the CNN [7] in 1998 with LeNet and its \\nbloom in 2012 with AlexNet was at the error rate of 15.3% \\nfollowed by ZF -net. The inventions of  GoogLeNe  and \\nVGGNet has made the error rate lower over time. An \\nexceptional milestone in this timeline was when ResNet \\nsurpassed the error rate of 3.6%, which was lower than that \\nof the human eye (5.1%) in 2015, proving that deep learning \\nmodels could surpass human ca pabilities.  \\n \\nA. Structure of CNN  \\nA typical CNN is structured with multiple layers: an \\ninput layer, a convolutional layer, an active layer, a pooling \\nlayer, a fully connected layer and finally, an output layer. \\nSome types of CNN models might include other laye rs for \\ndifferent purposes too . Figure 1 s hows the basic structure of \\na CNN architecture.  \\n \\n \\nFigure 1: The typical CNN structure with seven layers  \\n \\nSource:https://www.researchgate.net/publication/340102110_Hier\\narchical_Multi -View_Se mi-Supervised_Learning_for_Very_High -\\nResolution_Remote_Sensing_Image_Classification  \\n \\nThis multi -layered architecture is diverse in layers and \\nuses forward pass and error backpropagation calculations to \\nachieve the target's proficiency. Training this architecture to \\nbecome a model is a directed procedure that requires a \\ncollection of imager y data and their labels. Eventually, at the \\nend of the training process, the most suitable weights would \\nbe calculated to be used at the testing phase. These layers, as \\nmentioned above, could be further explained as follows.  \\n \\n \\n1) Input Layer  \\nThe input layer is used to initialize the input image data \\nand make all the available dimensions zero -centered. This \\nlayer is also responsible for normalizing the scale of all input \\ndata to a range within 0 and 1, which would help in \\naccelerating the speed of converging. This normalization is \\nalso helpful in reducing redundancy by whitening the data. \\nPrincipal Component Analysis (PCA) is done to degrade and \\ndecorate the available dimensions of the extracted data while \\nfocusing on key dimensions. [8] \\n \\n2) Convolutional Layer  As the layer, which is why CNN received its name, the \\nconvolutional layer is the most critical layer in a CNN \\nstructure.  Comprised of multiple element maps and many \\nneurons inside them, each of these neurons is created to \\nuntangle nearby qualities of various positions in the previous \\nlayer [9]. Many nearby associations and many mutual \\nattributes use a filter called CONV kernel, which slides on the \\noriginal image inputted to it. The CONV kernel calculates the \\nimage's component portrayal by multiplying and adding the \\nvalues of each pixel of the  local correlated data within it \\nbefore being added to the convolutional result. This so -called \\nrule of convolution enables the features of the image to be \\nextracted using the CONV kernel. The reason for filtering the \\nvarious parts of an image with the sam e CONV kernel is that \\nthis refers to shared weights. This usage of shared weights \\nenables neutral cells with the same features to be recognized \\nand classified into the same object type. Parameters such as \\nkernel size, depth, stride, zero -padding, and filte r quantity can \\nbe inputted onto this.  \\n \\n3) Active Layer  \\nThe active layer is the layer used to solve the problem of \\nthe vanishing gradient due to underfitting. This underfitting, \\nnonlinear problem is caused by the previous convolutional \\nlayer. One of the activ e layer functions such as Sigmoid, \\nTanh, the rectified Linear Unit (ReLu), the exponential \\nLinear Unit (ELU), Leaky ELU, or Maxout could be used in \\nsolving underfitting, following their usage [10]. Considering \\nthe converging speed, ReLu function has been the most \\npopula r although Sigmoid and Tanh functions are still \\ncommonly used due to their simplicity and efficiency.  \\n \\n4) Pooling Layer  \\nThe pooling layer's job is to efficiently reduce the \\ndimensions of the results sent from the convolutional layer. \\nThis is achieved by joini ng the neurons' outcome at one layer \\ninto a single neuron in the following layer, thus diminishing \\nthe elements of the component maps and incrementing the \\nstrength of selected extractions. Pooling layers are usually \\nsituated between two convolutional layer s and can be \\ncategorized into three distinct types based on their width: \\ngeneral pooling, overlapping pooling and Spatial Pyramid \\nPooling (SPP). A pooling layer is called a general pooling \\nlayer when its width is mainly equal to its stride. General \\npooling 's activities include max pooling and normal pooling. \\nWhen the most extreme incentives from each neuron group \\nfrom the previous layer are utilized, it is called max pooling. \\nWhen it is done for the normal incentives, it is referred to as \\nnormal pooling. Ov erlapping pooling is when the width is \\nlonger than the stride. Therefore, abnormal state attributes \\nfrom the input layer can be extracted and acquired by \\nstructuring a few convolutional layers along with a final \\npooling layer.  \\n \\n5) Fully Connected Layer  \\nOften the last layer before the output layer, the fully \\nconnected layer transmits data to the output layer while being \\nthe completely associated layer amongst the CNN layers. By \\nutilizing each neuron in the past layer and interfacing them to \\neach neuron on its o wn, it simplifies and speeds up the data \\ncalculation process. It being a completely associated layer \\nsaves no spatial data and is constantly trailed by a yield layer.  \\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2913c70d-f63b-47d6-b970-6ca500dc76d6', embedding=None, metadata={'page_label': '4', 'file_name': 'Yolo.pdf', 'file_path': '/content/data/Yolo.pdf', 'file_type': 'application/pdf', 'file_size': 427716, 'creation_date': '2024-02-09', 'last_modified_date': '2024-02-09', 'last_accessed_date': '2024-02-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\" \\n6) Other Layers  \\nApart from the different layers used in structuring a CNN \\nmodel mentioned a bove, some CNN models need additional \\nlayers to achieve the expected output. Layers such as dropout \\nlayers, regression layers come under this. Dropout layers are \\noften used to solve overfitting by avoiding majorly subjective \\nweights by updating weights of the neural cell knot with a \\ncertain probability (which is decided by the stochastic \\npolicy). Whereas, regression layer is used to classify features \\nusing a method such as logistic regression (LR), Bayesian \\nLinear Regression (BLR) and Gaussian Processes for  \\nRegression (GPR). The output of a regression layer is the \\nprobabilities of all the possible object types.  \\n \\nIII. TYPES OF OBJECT DETECTION ALGORITHMS  \\nAlgorithms available for object detection can be divided \\ninto two categories: classification -based algorithms a nd \\nregression -based algorithms.  \\n \\n1) Classification based algorithms  \\nClassification based algorithms are implemented in two \\nstages. The initial stage is the selection of region that is of \\ninterest  (RoI)  in the image. Then these regions are classified \\nwith the use of a convolutional neural network. This approach \\nof performing one stage prior to the other can be slow due to \\nthe need to run the prediction algorithm s on each region \\nselected in the first stag e. Few common examples for this \\ntype of algorithms are the Retina Net, Region -based CNN \\n(RCNN), the Fast -RCNN, Faster R -CNN and Mask -RCNN \\n(which is known to be a state -of-art under regional -based \\nCNN algorithms).  \\n \\n2) Regression -based algorithms  \\nRegression -based algorithms are implemented so that \\ninstead of selecting and singling out regions of interest in an \\nimage, they predict classes and their relevant bounding boxes \\nfor the whole image in one run through the model. Since \\nframe detection is treated as a regr ession problem, a complex \\npipeline is not necessary for regression -based algorithms. \\nFamous examples of this type of algorithms are the Single \\nShot Multibox Detector (SSD) and YOLO algorithms. Due to \\nthe simultaneousness of the detection and its nature of high \\nspeed (achieved with a tradeoff with accuracy), these are \\ncommonly used for real -time object detection. The detection \\nand understanding of the more popular YOLO algorithms \\nrequire an initial establishment of what will be predicted \\nbefore the models ar e used. The prediction would result in a \\nbounding box (specifying the Object's location) along with a \\nclass that has the highest probability amongst the established \\nset of classes.  \\n \\nIV. YOU ONLY LOOK ONCE (YOLO)  ALGORITHM  \\nYOLO is a novel approach to detect mu ltiple objects \\npresent in an image in real -time while drawing bounding \\nboxes around them. It passes the image through the CNN \\nalgorithm only once to get the output, thus the name. Although \\ncomparatively similar to R -CNN, YOLO practically runs a lot \\nfaster than Faster R -CNN because of its simpler architecture. \\nUnlike Faster R -CNN, YOLO can classify and perform bounding box regression at the same time. With YOLO, the \\nclass label containing objects, their location can be predicted \\nin one glance. Entirely devia ting from the typical CNN \\npipeline, YOLO treats object detection as a regression \\nproblem by spatially separating bounding boxes and their \\nrelated class probabilities, which are predicted using a single \\nneural network. This process of performing both boundi ng \\nbox prediction and class probability calculations is a unified \\nnetwork architecture that YOLO initially introduced.  \\nYOLO algorithm extends GoogLeNet equations to be used \\nas their base forwarding transport computation, assumably the \\nreason behind the spe ed and accuracy of YOLO's real -time \\nobject detection. In comparison with R -CNN architectures, \\nunlike running a classifier on a potential bounding box, then \\nreevaluating probability scores, YOLO predicts bounding \\nboxes and class probability for those boundi ng boxes \\nsimultaneously. This optimizes the YOLO algorithm and is \\none of the significant reasons why YOLO is so fast and less \\nlikely to have errors to be utilizable for real -time object \\npredictions.  \\nYOLO's architecture is similar to a typical convolutional  \\nneural network inspired by the GoogLeNet model for image \\nclassification. The network's initial layer first extracts the \\nimage's features, and the fully connected layers predict the \\noutput probabilities and coordinates. With 24 convolutional \\nlayers, two fu lly connected layers, 1x1 reduction layers and \\n3x3 convolutional layers, the full YOLO network model \\ncreated [12].  \\n \\nA. Unified Detection of YOLO  \\nYOLO is introduced as a unified algorithm as separate \\ncomponents merge into a single neural network as the final \\npipeline. For each bounding box to be predicted parallelly, the \\nfeatures of the entire image are globally reasoned. YOLO is \\ndesigned in such a way that it does its own end -to-end training \\nin real -time while keeping high -level average precision. To \\nachieve unified detection, YOLO first separates the input \\nimage into a S X S size grids. If the Object's center is being \\nplaced into the grid cell; the  grid cell tries object detection on \\nitself. Thus, every grid cell tries to estimate a bounding box \\nand their confidence scores across all classes trained to \\npredict. The predicted confidence scores will reflect how \\nconfident it is to provide each label an d bounding box to each \\nobject. Formally the confidence scores are defined as Pr \\n(Object) x IOUtruthpred . If an object has been found inside \\nthe cell, this confidence score will be equal to the intersection \\nover union (IOU) between the ground truth and the predicted \\nbox. If not, the confidence score would be equal to zero. The \\nunified detection outputs each confidence score to have five \\nparameters: w, y, w, h,  and confidence . The (x, y)  coordinates \\nrepresent the center of the box with respect to the grid cel l's \\nboundaries. As mentioned above, if the box's center does not \\nfall inside the grid cell, then the cell is not responsible for its \\nprediction. With each coordinate being normalized to be \\ncontained inside the range of 0 and 1, the estimated Object's \\nheigh t and width are calculated with respect to the entire \\nimage. According to Mauricio Menegaz in his article [11] the \\nprediction is of a few steps.  \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0c883cd3-48af-43db-aceb-5a11ebaa5edc', embedding=None, metadata={'page_label': '5', 'file_name': 'Yolo.pdf', 'file_path': '/content/data/Yolo.pdf', 'file_type': 'application/pdf', 'file_size': 427716, 'creation_date': '2024-02-09', 'last_modified_date': '2024-02-09', 'last_accessed_date': '2024-02-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\" \\nFigure 2: Example of how to coordinate parameters are calcula ted \\nin a 448 X 448 image with S = 3  \\nSource: https://hackernoon.com/understanding -yolo-f5a74bbc7967  \\n \\n Figure 2 depicts how the x coordinate of (220 -149)/149 is \\nnormalized as 0.48. And y  coordinate of (190 -149)/149 is \\nnormalized as 0.28. The width (w) of 224 is calculated as \\n224/448 = 0.50 with respect to the entire image. And height \\n(h) of 143 is calculated as 143/448 = 0.32 with respect to the \\nentire image.  \\nThe confidence score predicts  the IOU among the \\nprediction box and the ground truth box along with these \\nparameters. This confidence score reflects the presence or \\nabsence of an object of any class inside the bounding box. \\nAlong with these calculations, every grid cell having an objec t \\nalso estimates the conditional class probabilities, given as Pr \\n(Class(i) | Object) . This probability is conditioned on the grid \\ncell containing one object. Therefore, if no object is present \\non the grid cell, the loss function will not penalize it for a  \\nwrong class prediction. Since the network will only predict \\none set of class probabilities per cell regardless of the number \\nof boxes ( B), the total number of class probabilities could be \\ntaken as S X S X C . It is said that at the time of testing when \\nconfidence scores for each box is individually calculated, the \\nconditional class probabilities and the individual box \\nconfidence predictions are multiplied as; Pr (Class (i) | \\nObject) X Pr (Object) X IOUtruthpred = Pr (Class(i)) X \\nIOUtruthpred . The confidence  scores for each box reflect the \\nclass's possibility being shown inside the box and how exactly \\nthe Object fits the estimated box.  \\n \\nB. How YOLO Algorithm works?  \\nYOLO algorithm is an algorithm based on regression. It \\npredicts class probabilities of the object  and bounding boxes \\nspecifying the object’s location, for the entire image. The \\nbounding boxes of the object are described as: bx, by, the x, \\ny coordinates represent the center of the box relative to the \\nbounds of the grid cell. The bw, bh as the width and  height \\nare predicted relative to the whole image and the value c is \\nrepresenting the class of the object. Y OLO  takes the image as \\ninput and divides it into S x S grids (3 x 3). Then, image \\nclassification and object localization techniques are applied \\nto each grid of the image and each grid is given a label. The \\nYOLO  algorithm then checks every grid for an object and \\nidentifies its label and bounding boxes. The label of a grid \\nthat does not have an object is indicated as zero. Every \\nlabelled grid is defined  as S.S having 8 values. The 8 values namely are pc, bx, by, bw, bh, c1, c2, c3. Pc shows if a \\nparticular grid has an object or not. If an object is available, \\nthe pc is assigned 1 else 0. bx, by, bh, bw are bounding box \\nparameters of a grid and are only d efined if a proper object is \\navailable in that grid. c1, c2, c3 are classes. If the object is a \\ncar, then the value of c1, c2, c3 are 0,1,0 respectively [11].  \\n \\n \\nFigure 3: Example image with 3x3 grids.  \\nSource: https://jespublication.com/upload/2020 -110682.pdf  \\n \\nIn the example grid, a proper object cannot be \\nidentified from the first grid. Therefore, pc value is 0 and \\nbounding box parameters need not be assigned as there is no \\ndefined object. Class probability cannot be identified as there \\nis no proper object (Fig ure 4). The 6th grid has a proper object \\nand therefore pc value is assigned 1 and bounding boxes for \\nthe object are bx, by, bw and bh. Since the object is a car, the \\nclasses for the grid are 0,1,0 (Figure 5) [11].  \\n \\n \\nFigure 4:Boundi ng box and class values for grid 1  \\nSource: https://jespublication.com/upload/2020 -110682.pdf  \\n \\n \\nFigure 5:Bounding box and class values for grid 6  \\nSource: https://jespublication.com/upload/2020 -110682.pdf  \\n \\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='71ac250a-1c6e-4dd2-9456-fe44fe22199f', embedding=None, metadata={'page_label': '6', 'file_name': 'Yolo.pdf', 'file_path': '/content/data/Yolo.pdf', 'file_type': 'application/pdf', 'file_size': 427716, 'creation_date': '2024-02-09', 'last_modified_date': '2024-02-09', 'last_accessed_date': '2024-02-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"The matrix is defined as S  x S x 8, where S x S \\nrepresents the entire grid size, image gets divided into and 8 \\nindicates the total count of p c, bx, by, bw, bh, c1, c2, c3 \\nvalues. Bounding boxes differ for each grid depending on the \\nposition of objects in the relative grid. If more than two grids \\nhave the same object, then the grid cell that has the center of \\nthe object is used to detect that ob ject. For a precise \\nidentification of the object, two methods can be used ; 1. \\nIntersection over Union (IOU) 2. Non -Max Suppression [11]. \\nIn IOU, actual and estimated bounding box values are used \\nand the IOU of both values are computed using the following \\nformulae.  \\n \\n                             IOU = Intersection Area  \\n                                       ------------------------  \\n                                        Union Area  \\n \\nIt is better if the computed  IOU is greater than a \\nthreshold value (an assum ed value for increasing the accuracy \\nof the detected object.) 0.5  [11].   \\nIn Non -Max Suppression, the next method, high possibility \\nboxes are used and the boxes with high  IOU values are \\nsuppressed [11]. This process is followed many times until a \\nbox is considered as the bounding box for the object.  Each \\ngrid cell also predicts ‘c’ conditional class probabilities for \\nthe object in that grid. These probabilities are conditioned on \\nthe grid cell containing an object. Only one set of class \\nprobabilities is predicted for a grid cell, regardless of the \\nnumber of bounding boxes for tha t grid cell. [12] \\n \\n \\nFigure 6: Complete process of Object detection b y YOLO  \\nSource: https://jespublication.com/upload/2020 -110682.pdf  \\n \\nV. YOLO  VERSIONS  \\n• YOLOv1  \\nBase YOLO is also called YOLO Version 1[10]. It \\ndetects the object basing it as a regression problem. A single \\nconvolutional network predicts multiple bounding boxes and \\nclass probabilities for all the grid cells simultaneously. The \\ninput image is divided into S x S grids. If the center of a \\nproper object falls into a grid cell, that grid cell will be \\nconsidered in detecting that object. Each grid cell predicts N \\nbounding boxes and confidence scores for the boxes and c \\nclass probabilities. At test time, the  class probabilities and the \\nindividual box confidence predictions are multiplied resulting in class -specific confidence scores for each \\npredicted box. These scores encode both the probability of \\nthat class appearing in the box and how well the predicted \\nbox fits the object (Figure 6). These predictions are encoded \\nas an S × S × (N ∗ 5 + c) tensor. The width(bw) and the \\nheight(bh) are predicted relative to the whole image. And that \\nis why YOLOv1 uses Nx5 for calculating tensor. [10]  \\n \\n Pr(Class i |Object) ∗ Pr(Object) ∗ IOU pred = Pr(Class i \\n) ∗ IOU pred.  \\n \\nThese confidence scores reflect how confident the model \\nis in ensuring that the predicted box contains an object and \\nhow accurate the model thinks the box around the different \\nobjects is. Confidence score i s defined as:  \\n   Pr(Object) * IOUtruthpred  \\n \\nYOLOv1’s network has 24 convolutional layers as \\nopposed to YOLOv2, which has 19 layers [10]. For \\nevaluating YOLO model on the PASCAL VOC detection \\ndataset, these values are used: S=7, therefore a 7x7 grid. N=2, \\nnumber of bounding boxes. The PASCAL VOC dataset has \\n20 labelled classes so c=20. Therefore YOLOv1’s final \\nprediction is a 7x7x(5x2+20)=7x7x30 tensor. Here only 98 \\nbounding boxes per image is used [10], [12]  \\n \\n• YOLOv2  \\nYOLO Version 2 is an improved version of the existing \\nYOLO algorithm. The speed of detection performance \\nremains same while the mAP value increased compared to \\nYOLOv1’s Map value of 63.4. New multi -scale training \\nmethod can be used to run the YOLOv2 run at various sizes \\noffering improvements in a ccuracy and speed in prediction.  \\nYOLOv2 adds a list of significant solutions to increase mAP. \\nBatch Normalization preprocesses the input data. High \\nResolution Classifier from YOLOv1’s 224x224 to 448x448 \\nraises the mAP by 4%. Its neural network has 19 \\nconvo lutional layers compared to the YOLOv1 which has 24. \\nYOLOv2 adopts convolutional with anchor boxes and \\nincreases each grid cell’s resolution from YOLOv1’s 7x7 to \\n13x13. It also has only one bounding box for each grid cell. \\nFinally, YOLOv2 adds a pass -throu gh layer to get the \\nextracted features from the former layer and combine them \\nwith the original final output features, so that the ability of \\ndetecting the small object would be enhanced. In this mean, \\nYOLOv2 raises the mAP by 1% [10].  \\n \\nVI. STRENGTHS AND WEAKN ESSES OF YOLO  \\nYOLO is the state -of-art real -time object detection \\nalgorithm that surpasses the previous CNN detection speed \\nlimits while maintaining a good balance between speed and \\naccuracy. YOLOv2, the latest version of YOLO achieving a \\nmean Average Prec ision (mAP) rate of 76.8 at 67 Frames per \\nSecond (FPS) and 78.6 mAP rate at 7 6 FPS, outperforms \\nregional -based algorithms such as Faster R -CNN in both \\nspeed and accuracy. Another great strength in YOLO is its \\nglobal reasoning skills that encode the context ual information \\nabout the whole image rather than a specific region. Along \\nwith these global reasoning skills, the ability to predict false \\npositives in the background increases, improving the \\nalgorithm's reasoning skills as a whole. Lastly, with YOLO's \\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='154c5e30-c9fb-44f0-b91e-6f50a731f36d', embedding=None, metadata={'page_label': '7', 'file_name': 'Yolo.pdf', 'file_path': '/content/data/Yolo.pdf', 'file_type': 'application/pdf', 'file_size': 427716, 'creation_date': '2024-02-09', 'last_modified_date': '2024-02-09', 'last_accessed_date': '2024-02-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"ability to learn basic representations of the labelled objects, it \\nhas outperformed other detection methods, including \\n(Deformable Part Model) DPM and R -CNN when \\ngeneralizing natural images amongst images like artwork. \\nDue to YOLO's generalizability and appl icability in new \\ndomains and unexpected outputs, it is considered one of the \\nbest object detection algorithms in the domain.  \\nAlthough YOLO has many unique strengths, it also \\nhas weaknesses. One of the notable weaknesses of YOLO is \\nits spatial constraints on bounding boxes. These spatial \\nconstraints are held due to each cell being able to predict only \\ntwo boxes and one class. It limits the number of predictable \\nobjects nearby to each other in groups (such as the \\nrecognition of a flock of birds, a basket of similar fruits). Due \\nto only being taught through input data, YOLO also has a \\nweakness in generalizing objects in unusual or new aspect \\nratios. However, this could be considered as more of a general \\nproblem in the object detection domain. Since the model o nly \\nuses relatively coarse features for prediction, the architecture \\nhas a few down sampling layers from the input images, which \\ncan be mentioned as a general weakness  of YOLO . Compared \\nwith an ideal algorithm, YOLO's loss function (which \\napproximates the detection performance) treats errors with \\nthe same loss despite its object boxes' size. This is not \\nadvantageous since a small error in a small box is not \\nequivalent to a small error in a large box, which has a more \\nsignificant effect on the Intersection o ver Union (IOU), \\ngiving out incorrect localizations in the end. Therefore, th ese \\ncould be taken as some of the areas YOLO algorithm could \\nimprove upon.   \\n \\nVII. CONCLUSION AND FUTURE SCOPE  \\nThis paper reviews the fundamental structure of CNN \\nalgorithms and an overview of YOLO's real -time object \\ndetection algorithm. CNN architecture models can remove \\nhighlights and discover objects in each given image. When \\nproperly used, CNN models can solve deformity \\nidentification, instructive/ learning application creation etc. \\nWhen in comparison with other CNN algorithms, YOLO has \\nmany advantages in practice. Being a unified object detection \\nmodel that is simple to construct and train in correspondence \\nwith its simple loss -function, YOLO can train the entire model \\nin parallel. The second major version of YOLO, YOLOv2, \\nprovides the state -of-art, best tradeoff between speed and \\naccuracy for object detection. YOLO is also better at \\ngeneralizing Object represen tation compared with other object \\ndetection models and can be recommended for real -time \\nobject detection as the state -of-art algorithm in object \\ndetection. With these marks, it is acknowledgeable that the \\nfield of object detection has an expanding, great f uture ahead.  \\nACKNOWLEDGEMENT  \\nThe authors would like to thank Dr Dharshana \\nKasthurirathna of the Faculty of Computing, SLIIT, to write \\nthis review paper as an assignment. The expertise of everyone \\nwho improved this study in numerous ways is also gratefully  \\nappreciated.  REFERENCES  \\n[1] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, \\n“Gradient -Based Lerning Applied to Document \\nRecognition,” proc. IEEE , 1998, [Online]. Available: \\nhttp://ieeexplore.ieee.org/document/726791/#full -\\ntext-section.  \\n[2] T. F. Gonzalez, “Handbook of approximation \\nalgorithms and metaheuristics,” Handb. Approx. \\nAlgorithms Metaheuristics , pp. 1 –1432, 2007, doi: \\n10.1201/9781420010749.  \\n[3] K. Simonyan and A. Zisserman, “Very deep \\nconvolutional networks for large -scale image \\nrecognition,” 3rd Int. Conf. Learn. Represent. ICLR \\n2015 - Conf. Track Proc. , pp. 1 –14, 2015.  \\n[4] C. Szegedy et al. , “Going Deeper with \\nConvolutions,” 2015, doi: 10.1002/jctb.4820.  \\n[5] K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual \\nLearning for Image Recognition,” doi: \\n10.1002/chin.200650130.  \\n[6] J. Hosang, R. Benenson, P. Dollar, and B. Schiele, \\n“What Makes for Effective Detection Proposals?,” \\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 38, no. \\n4, pp. 814 –830, 2016, doi: \\n10.1109/TPAMI.2015.2465908.  \\n[7] A. S. R. H. A. J. S. S. Carlsson, “CNN Feat ures off -\\nthe-shelf: an Astounding Baseline for Recognition,” \\nProc. IEEE Conf. Comput. Vis. Pattern Recognit. \\nWork. , vol. 7389, pp. 806 –813, 2014, doi: \\n10.1117/12.827526.  \\n[8] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \\n“ImageNet Classification with Deep Convolutional \\nNeural Networks,” [Online]. Available: \\nhttps://www.cv -\\nfoundation.org//openaccess/content_cvpr_workshop\\ns_2014/W15/papers/Razavian_CNN_Features_Off -\\nthe-Shelf_2014_CVPR_paper.pdf.  \\n[9] T. Guo, J. Dong, H. Li, and Y. Gao, “Simple \\nconvolutional neu ral network on image \\nclassification,” 2017 IEEE 2nd Int. Conf. Big Data \\nAnal. ICBDA 2017 , pp. 721 –724, 2017, doi: \\n10.1109/ICBDA.2017.8078730.  \\n[10] J. Du, “Understanding of Object Detection Based on \\nCNN Family and YOLO,” J. Phys. Conf. Ser. , vol. \\n1004, no. 1, 2018, doi: 10.1088/1742 -\\n6596/1004/1/012029.  \\n[11] Mauricio Menegaz, “Understanding YOLO – Hacker \\nNoon,” Hackernoon . 2018.  \\n[12] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, \\n“You only look once: Unified, real -time object \\ndetection,” 2016, doi: 10.11 09/CVPR.2016.91.  \\n \\nView publication stats\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt=\"\"\"\n",
        "You are a Q&A assistant. Your goal is to answer questions as\n",
        "accurately as possible based on the instructions and context provided.\n",
        "\"\"\"\n",
        "# Default format supportable by Llama2\n",
        "query_wrapper_prompt=SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")"
      ],
      "metadata": {
        "id": "64OBZSpXVoko"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face Login\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hjXeS5qWbjw",
        "outputId": "2e917fff-3d3a-48e6-a7ac-fc1085e25508"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: read).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the Llama2 model from Hugging Face\n",
        "import torch\n",
        "\n",
        "llm=HuggingFaceLLM(\n",
        "    context_window=4096,\n",
        "    max_new_tokens=256,\n",
        "    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False}, # temperature value ranges between 0 to 1. It indicate the creativity of your model.\n",
        "    system_prompt=system_prompt,\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    device_map=\"auto\",\n",
        "    # uncomment this if using CUDA to reduce memory usage\n",
        "    model_kwargs={\"torch_dtype\": torch.float16, \"load_in_8bit\": True} # Quantizing from 16 bit to 8 bit\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "cb946a606d114ff28ecf5b360c81dd40",
            "0164dfaa69464489a51e4630bdd669fa",
            "0f17ec0893fa4380a37aa7d657f9565b",
            "402200f1e27a4e0aa98f2bf0be3ff496",
            "dd08c0240e0841799a6c215461b7ad41",
            "8f70184cfad442df9baf28c4ca3cab5f",
            "9f9c70cb370f4d9c991f78dd525d81a5",
            "22d28d6933f64e0280347aafafd03e42",
            "b93997fcbd2d4f07a328fd63ceb8ceb1",
            "64fa7ce5e6044cfab105dd4493030130",
            "b6d2f5aad89c4f0082496a48307ec0e9",
            "4ba772bbc2b1478096db7b1bc44c69f5",
            "86f1868295bc4c60a48954c6a01e2321",
            "5614601f3b1f4aa784494ace6c49b03f",
            "a6659dec177b4c49bfa87721fd5fc6c7",
            "b882e534eb774061a5c1ae981a1f2867",
            "6578105ba2804da88bb315cf09e215ae",
            "d342696744724ede85ba8f6d082dd66d",
            "898ccd6e85164d83840d907e92ef8a0e",
            "7583f6088a53439a895452992542b7cc",
            "866bc7e5873f44429f3c936b31729869",
            "83236ac70dd941138664f088ef7eb7cc",
            "869f1b0de6294e70b48ca0d520a96e06",
            "dc26823500214cc085c37bf1f5de1444",
            "9251cccf6e554926962bb8cceaf0af72",
            "53cee1b1eb8f4a558f45625c82438879",
            "747e234a9e694edc9a9c9e842c20fc26",
            "0bf0b78b6d5345aaa4d60d395a471788",
            "aa997935c3bc41389e59f5d2ebef12df",
            "2aaa72e50c044fc2aee495ee220223e0",
            "8270bd7cf97644f3928a64da78fce2c0",
            "29b13c7f5e654969b5e2da3db9011dea",
            "ba42cd53bbdd4400afb331fa10206c52",
            "e063d50ab9d9434e81aca0b2dd9c07a5",
            "1b37212aa28045b8b5c06447c4fe0d3a",
            "f8ec695136c243f9a773bb0a685775c9",
            "70fd435dd0dd4bf6a6d2bcc1e2f20344",
            "9353a274951c437293d83ffb954eed90",
            "e2b852f61e6643a394f3b25f0de4c541",
            "936a4f9d5e2c4f4cbcf3aa1acd25d978",
            "4f4aec26fca2475dba6fdbd499836f64",
            "e6099b66b10247e58906158487a1db55",
            "a20864cffaf8409bbc1d08af41483ad4",
            "8fc9d10ac6b943e0a21dd757025240f6",
            "affd5c74654e43839946b1a98110a201",
            "6cf83d42770b413fa8cc6e8cd1f71e95",
            "58b0649a165b42cf888afbaba04d6ab4",
            "874e96254eaa4b978d51081f3cad491d",
            "7969158f4f1a4b8aa1241e35796100d2",
            "5320c1c3bc8642189fac59b22e6ff692",
            "684c1045f8d04138bed95ae6c33051eb",
            "c8f0f67e3f994916af349fe6658108cb",
            "722a812a5b02486dbe49e43604f9eb5c",
            "80cd85becbb748358ad192ad246fde93",
            "c5f0d33ad8e144d8927f365c3d954738",
            "1efd98b4a4c5485181dd594af8a4d6b7",
            "c85d3eeb3db04eaf818b513cfb626de1",
            "130a5564821743668c97227e267b1cdd",
            "fb1ef171dae74ed9aada80cabe9f5c57",
            "a79d24b9370e442d8575ac3cad48d3a5",
            "5085c44f28ee494b813d265022491dba",
            "97490bd9e28641bf93596668a48387f9",
            "2f39a6a9f0e54d2f99441833e7ba9933",
            "63541f66f28540ec91864cdad1c43ba9",
            "a684657b6c934805a7409c0f6bd7079c",
            "19b6b36688ba4e7087af48faf5b8bbba",
            "a57486967d55491483b331468788d280",
            "433ae3bb5f674e90a8971e9aacefa1e1",
            "2874955e26804dae951ab4aba5de0e02",
            "1e13ebe2ae184505bbcf692cf5c72975",
            "a2d2261245c4497db660f4207c777780",
            "8bcae2ac8143425c8df9775d7db8d6da",
            "cc1e9884a6d14cc692ed7a482fc8b818",
            "81d6094480454c09808a067c252f7948",
            "8b82fe2c6c134329be35a0d7048cad94",
            "eebbe7a3129048ccb5a4549024ff5176",
            "38740f0a737f41a0893656efe1faa4e4",
            "5fcd892370ce45eb902f8b44a944a2d7",
            "04f8611d8a134687bae4ee9f47eca01d",
            "1c54fa0bf4614614b15b2cfe4ce459ac",
            "d3a671e6d99043558d280e99b187c626",
            "01df4917a7d24b4a859f74e041c86bbe",
            "05b6ec31702c46929a94c85e647b33a0",
            "6b027178c1bf46439195e38eec576781",
            "9cd6061ba08d489d928401827c67768b",
            "e6b8d546521542bfa76be537250fa227",
            "d768ca2f76564971b21ef7538e410da8",
            "49d45175ff904a0c8cb0e0803e050bcb",
            "d0206ee5e2994028bb669e69d2ba7dcc",
            "cd7c13a2cf0946a19c7531eda2eef7c3",
            "3686dc6780a749e29021efffa8ba7cd5",
            "248451229550494b8a91b89b8dc264e7",
            "03b16241c3114e6a9256a4a29b255c80",
            "807f1ffb60d44f9790cac8f5411f8c3e",
            "ccc8f4193f564964b92cb39ac8edee3e",
            "9b09d3309c5e4c499b1bbb42c65635e9",
            "7893ef8d7efd4d279928e4183c975b91",
            "e2d7dd1834b34a40b13ba1f3c6601cdf",
            "89ebfe22a13a46a79923da5141d60c66",
            "be1effc4e5934bc0a661da852f2e779b",
            "964ae5cfd742416fa6c05428da6bf9e4",
            "b2e7c8572bd843f593580a046b019779",
            "b629ec8f234c4dc39b31f43f026f4e75",
            "382b53f506d64503967ad73b03d7503d",
            "dd4253b8013248ec95b30791acf1ad14",
            "6d448e67d9ff45c0ba19ef0606715081",
            "2f4ec12e05494bafa4c045650940dbb4",
            "968616924fba4ca5bf0fd84e4b201180",
            "fdeac5a92d344864b4c10d4f674de087",
            "6f461fb8049c402d8ac27e0280408f62",
            "441912bc03d643339a9970135419585d",
            "979e84405c334ab9bf8475f9b61cf0ee",
            "22ad924f3d0c48619ee34a70c954d3b0",
            "9c56397dc6c6444fbdf1793cde031af6",
            "71c94939266c45039d28efdf5eab08cf",
            "0784f293c77247ee9f3ea85a991600ad",
            "feea17c4e542407187c367550610b839",
            "e2d43bb4f34c49a0a2a17fbf5040669c",
            "16e260e165344c98a524e436f9a4e6e1",
            "0b94e6c769424a678902ad21d713a349",
            "cfd28511004a4ee7b5e32dcbf5f5dc67"
          ]
        },
        "id": "dsVOJ9s6X52V",
        "outputId": "aea6e0b4-d8fe-4bbb-daf3-1a1f3ce74657"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cb946a606d114ff28ecf5b360c81dd40"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ba772bbc2b1478096db7b1bc44c69f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "869f1b0de6294e70b48ca0d520a96e06"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e063d50ab9d9434e81aca0b2dd9c07a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "affd5c74654e43839946b1a98110a201"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1efd98b4a4c5485181dd594af8a4d6b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a57486967d55491483b331468788d280"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5fcd892370ce45eb902f8b44a944a2d7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d0206ee5e2994028bb669e69d2ba7dcc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "be1effc4e5934bc0a661da852f2e779b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "441912bc03d643339a9970135419585d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from llama_index import ServiceContext\n",
        "from llama_index.embeddings import LangchainEmbedding\n",
        "\n",
        "# Combing langchain with llama2\n",
        "embed_model=LangchainEmbedding(\n",
        "    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\"))"
      ],
      "metadata": {
        "id": "UAi3aHKMcnVY"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "service_context=ServiceContext.from_defaults(\n",
        "    chunk_size=1024,\n",
        "    llm=llm,\n",
        "    embed_model=embed_model\n",
        ")"
      ],
      "metadata": {
        "id": "k5nkmGnthpRA"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "service_context"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XL2MgdX-k92L",
        "outputId": "e5df9b0e-f720-4438-a072-9d1153fe5452"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ServiceContext(llm_predictor=LLMPredictor(system_prompt=None, query_wrapper_prompt=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>), prompt_helper=PromptHelper(context_window=4096, num_output=256, chunk_overlap_ratio=0.1, chunk_size_limit=None, separator=' '), embed_model=LangchainEmbedding(model_name='sentence-transformers/all-mpnet-base-v2', embed_batch_size=10, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7a38924b1240>), transformations=[SentenceSplitter(include_metadata=True, include_prev_next_rel=True, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7a38924b1240>, id_func=<function default_id_func at 0x7a3944f670a0>, chunk_size=1024, chunk_overlap=200, separator=' ', paragraph_separator='\\n\\n\\n', secondary_chunking_regex='[^,.;。？！]+[,.;。？！]?')], llama_logger=<llama_index.logger.base.LlamaLogger object at 0x7a3898bc8790>, callback_manager=<llama_index.callbacks.base.CallbackManager object at 0x7a38924b1240>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the entire data into indexes\n",
        "index=VectorStoreIndex.from_documents(documents, service_context=service_context)"
      ],
      "metadata": {
        "id": "Xryh2SRSl98s"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n12LH4ELl94q",
        "outputId": "86e1c08e-93f1-4214-85eb-633fc2a50085"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<llama_index.indices.vector_store.base.VectorStoreIndex at 0x7a389a86ae60>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the above index into query engine\n",
        "query_engine=index.as_query_engine()"
      ],
      "metadata": {
        "id": "RZpbnDJ2mjeO"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=query_engine.query(\"How YOLO Algorithm works?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPDudSMNmjae",
        "outputId": "889cdd4c-c8ba-4c04-a154-b2f57c31b1dd"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkKlcRv3mjWt",
        "outputId": "e91f2d09-7082-4c29-b9df-6eefd4c04034"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOLO algorithm works by first separating the input image into a S X S size grids. Each grid cell tries object detection on itself, and the predicted confidence scores reflect how confident it is to provide each label and bounding box to each object. The unified detection outputs each confidence score, which includes the (x, y) coordinates representing the center of the box with respect to the grid cell's boundaries. The estimated object's height and width are calculated with respect to the entire image.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response=query_engine.query(\"What are the different kinds of attention?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txe12_LgmjTK",
        "outputId": "0dff8eee-611a-4617-d924-51616292b841"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMctsVbQrCzj",
        "outputId": "b577d53f-b45e-48d1-9026-1c47d3f5f7b0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are several different kinds of attention, including:\n",
            "\n",
            "1. Sustained attention: the ability to focus on a task for a prolonged period of time.\n",
            "2. Selective attention: the ability to focus on a specific task or stimulus while ignoring others.\n",
            "3. Alternating attention: the ability to switch focus between two or more tasks or stimuli.\n",
            "4. Dividing attention: the ability to perform multiple tasks simultaneously.\n",
            "5. Executive attention: the ability to control and manage attention, including the ability to switch between tasks and to focus on a specific task.\n",
            "6. Attentional control: the ability to regulate and manage attention, including the ability to focus on a specific task and to switch between tasks.\n",
            "7. Attentional flexibility: the ability to switch between different tasks and to adapt to changing conditions.\n",
            "8. Attentional capacity: the ability to process and store information in working memory, which is the ability to hold and manipulate information in one's mind for a short period of time.\n",
            "\n",
            "These are some of the different kinds of attention, and there may be others depending on the context and the specific task or situation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GbxZhoa8rFAL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}